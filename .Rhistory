}
}
#write.csv(comments, file = "Data/comments.csv", row.names = FALSE)
#### Download Attachments ####
attachList <- subset(comments, attachmentCount > 0)
errflag = 1
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
test <- HEAD(attachUrl)
if (test$status_code==200){
tmp <- test$headers$`content-disposition`
file <- substr(tmp, nchar(tmp)-1, nchar(tmp)-1)
file <- str_extract(tmp, "\\.[A-Za-z]{3,4}")
download.file(attachUrl, paste0("files/",  attachList[comment, "documentId"], "-", doc, file), mode="wb")
} else if (errflag==1){
errorLog = data.frame(docId = attachList[comment, "documentId"], error = test$status_code)
errflag = errflag+1
} else {
errorLog[errflag, 1] = attachList[comment, "documentId"]
errorLog[errflag, 2] = error = test$status_code
errflag = errflag+1
}
}
}
write.csv(errLog, file = "Data/attachmentDownloadErrorLog.csv", row.names = FALSE)
write.csv(errorLog, file = "Data/attachmentDownloadErrorLog.csv", row.names = FALSE)
View(errorLog)
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
# Load libraries
#library(readxl)
#library(rvest)
#library(scrapeR)
library(jsonlite)
library(httr)
library(stringr)
#library(tidyverse)
#### API Data Call ####
dktid = "CMS-2017-0163"
api_key = "bbPnmY2FqvoazRuHseN0liEsWh0qI255CgJTsPAo"
#API call to get the number of record calls needs. API limits pull to 1000 records
countUrl = paste0("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=", dktid)
recCount <- fromJSON(countUrl)
pageCount <- ceiling(recCount$totalNumRecords/100)
for (i in 1:pageCount){
pageUrl = paste0("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100)
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comments, tmp)
}
}
write.csv(comments, file = "Data/comments.csv", row.names = FALSE)
#### Download Attachments ####
attachList <- subset(comments, attachmentCount > 0)
errflag = 1
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
test <- HEAD(attachUrl)
if (test$status_code==200){
tmp <- test$headers$`content-disposition`
file <- substr(tmp, nchar(tmp)-1, nchar(tmp)-1)
file <- str_extract(tmp, "\\.[A-Za-z]{3,4}")
download.file(attachUrl, paste0("files/",  attachList[comment, "documentId"], "-", doc, file), mode="wb")
} else if (errflag==1){
errorLog = data.frame(docId = attachList[comment, "documentId"], error = test$status_code)
errflag = errflag+1
} else {
errorLog[errflag, 1] = attachList[comment, "documentId"]
errorLog[errflag, 2] = test$status_code
errflag = errflag+1
}
}
}
write.csv(errorLog, file = "Data/attachmentDownloadErrorLog.csv", row.names = FALSE)
source("helper_functions.R")
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
# Load libraries
library(readxl)
library(tidyverse)
source("helper_functions.R")
#### Load Raw Datasets ####
comLoc <- "C:/Data/Comments/Testing"
#Comment Report from FDMS
comReport <- read_excel(paste0(comLoc, "/report.xlsx"), sheet = 1)
colnames(comReport) <- make.names(colnames(comReport))
#Comment Report from FDMS
attReport <- read_excel(paste0(comLoc, "/report.xlsx"), sheet = 2)
colnames(attReport) <- make.names(colnames(attReport))
#Comment Dataset from Data_Download.R
comments <- read_csv("Data/comments.csv")
#Map of Comments, Commenters, Commenter Categories
map <- read_excel(paste0(comLoc, "/map.xlsx"))
colnames(map) <- make.names(colnames(map))
#Text Extract of Comment Attachments
attachExtract <- read_excel("Data/CMS-2017-0163-TExtract.xlsx")
colnames(attachExtract) <- make.names(colnames(attachExtract))
comReport$Site_Key <- substring(comReport$Email.Address, regexpr("@", comReport$Email.Address) + 1)
#### Load Text Extract ####
attachExtract <- attachExtract %>%
mutate(Comment.ID = substr(File.Name,1,18),
Document.ID = substr(File.Name, 1, regexpr("\\.", File.Name) - 1)) %>%
select(Comment.ID, Document.ID, Text, Page.Number) %>%
left_join(comReport, by = c("Comment.ID" = "Document.ID")) %>%
left_join(map, by = c("Comment.ID" = "Document"))
commentsDf <- comments %>%
mutate(Document.ID = paste0(documentId, "-0"),
Comment.ID = documentId,
Text = commentText,
Page.Number = 1) %>%
select(Comment.ID, Document.ID, Text, Page.Number) %>%
left_join(comReport, by = c("Comment.ID" = "Document.ID")) %>%
left_join(map, by = c("Comment.ID" = "Document")) %>%
union(attachExtract)
#### Filter Out Message Comments Only Referencing Attachments Comments ####
# Create test file to review and find word count cutoff
testComment <- commentsDf %>%
filter(!(word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")))
write.csv(testComment, file = "Data/testAttacted.csv")
# Apply word filter with word count cutoff
commentsDf <- commentsDf %>%
filter(!((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")) & wordcount(Text) < 150))
#### Export Cleaned File for Text Mining
write.csv(commentsDf, file = "Data/commentsDf.csv", row.names = FALSE)
View(map)
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
baseStart = 66
#Load libraries
library(tidyverse)
library(readxl)
library(tidytext)
library(tm)
library(SnowballC)
library(topicmodels)
baselineDoc <- read_excel("Data/AN_Part2 - V2.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
baselineDoc$Text <- gsub("[^A-z0-9 ]", "", baselineDoc$Text)
bigram_count <- baselineDoc %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
View(bigram_count)
bigram_count <- baselineDoc %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2))
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
count(unique(commentsDf$Comment.ID))
length(unique(commentsDf$Comment.ID))
bigram_count <- baselineDoc %>%
union(commentsDf) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
#    union(commentsDf) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
View(baselineDoc)
?select
View(commentsDf)
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
union(., select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
select(Document.ID, Text) %>%
#    union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% cms_stop) %>%
filter(!word2 %in% cms_stop) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram_count <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% c(stop_words$word, cms_stop)) %>%
filter(!word2 %in% c(stop_words$word, cms_stop)) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
arrange(desc(n))
bigram <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% c(stop_words$word, cms_stop)) %>%
filter(!word2 %in% c(stop_words$word, cms_stop)) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(n/length(unique(commentsDf$Comment.ID)) > .08)
bigram <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
#filter(!word1 %in% c(stop_words$word, cms_stop)) %>%
#filter(!word2 %in% c(stop_words$word, cms_stop)) %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(n/length(unique(commentsDf$Comment.ID)) > .08)
View(bigram)
bigram <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(n/length(unique(commentsDf$Comment.ID)) > .08)
baselineDoc_section <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section))
baselineDoc_section <- baselineDoc_section %>%
filter(Paragraph.Number < 626 | Paragraph.Number > 695)
baseline_unigram <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baseline_bigram <- baselineDoc_section %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(bigram, word1, word2, sep = " ") %>%
inner_join(bigram)
baseline <- union(baseline_unigram, baseline_bigram)
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total) %>%
filter(total > 5)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
baseline_bigram <- baselineDoc_section %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ") %>%
inner_join(bigram)
bigram <- baselineDoc %>%
select(Document.ID, Text) %>%
union(select(commentsDf, Document.ID, Text)) %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
count(word1, word2, sort = TRUE) %>%
unite(word, word1, word2, sep = " ") %>%
filter(n/length(unique(commentsDf$Comment.ID)) > .08)
baseline_bigram <- baselineDoc_section %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ") %>%
inner_join(bigram)
baseline <- union(baseline_unigram, baseline_bigram)
View(baseline_bigram)
View(baseline_unigram)
baseline_bigram <- baselineDoc_section %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ") %>%
filter(word %in% bigram$word)
baseline <- union(baseline_unigram, baseline_bigram)
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total) %>%
filter(total > 5)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
View(base_tf_idf)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
base_lda <- LDA(base_dtm,
k = length(unique(base$section)),
control = list(seed = 1234))
save(base_lda, file = "Models/lda_test.rda")
save(base_lda, file = "Models/lda_test.rda")
base_doc_topics <- tidy(base_lda, matrix = "gamma")
base_topics <- tidy(base_lda, matrix = "beta")
model_top_terms <- base_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
write.csv(model_top_terms, file="Data/modelingtopterms.csv")
topic_map <- rbind(topic_map1, topic_map2) %>%
distinct() %>%
select(-gamma)
#### Create Topic Map ####
topic_map1 <- base_doc_topics %>%
group_by(document) %>%
filter(gamma == max(gamma))
topic_map2 <- base_doc_topics %>%
group_by(topic) %>%
filter(gamma == max(gamma))
topic_map <- rbind(topic_map1, topic_map2) %>%
distinct() %>%
select(-gamma)
View(topic_map)
View(model_top_terms)
model_top_terms <- base_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta) %>%
inner_join(topic_map)
write.csv(model_top_terms, file="Data/modelingtopterms.csv", row.names = FALSE)
View(base_lda)
?LDA
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
mutate(word = wordStem(word))
comment.bigrams <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
anti_join(stop_words, by = c("word1" = "word")) %>%
anti_join(stop_words, by = c("word2" = "word")) %>%
anti_join(cms_stop, by = c("word1" = "word")) %>%
anti_join(cms_stop, by = c("word2" = "word")) %>%
filter(!(str_detect(word1, regex("^http"))),
!(str_detect(word1, regex("^www"))),
!(str_detect(word2, regex("^http"))),
!(str_detect(word2, regex("^www")))) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ") %>%
filter(word %in% bigram$word)
comments <- union(comment.words, comment.bigrams)
#### TF-IDF Testing ####
word_count <- comments %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
