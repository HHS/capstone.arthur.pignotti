.libPaths( c("C:/R/Packages", .libPaths()) )
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
url = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=CMS-2017-0163", sep ="")
api_key = "bbPnmY2FqvoazRuHseN0liEsWh0qI255CgJTsPAo"
url = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=CMS-2017-0163", sep ="")
library(jsonlite)
test <- fromJSON(url)
View(test)
test$totalNumRecords
recCount <- fromJSON(countUrl)
countUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=CMS-2017-0163", sep ="")
recCount <- fromJSON(countUrl)
recCount$totalNumRecords
pageCount <- ceiling(recCount$totalNumRecords/1000)
api_key = "bbPnmY2FqvoazRuHseN0liEsWh0qI255CgJTsPAo"
dktid = "CMS-2017-0163"
#API call to get the number of record calls needs. API limits pull to 1000 records
countUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=", dktid, sep ="")
recCount <- fromJSON(countUrl)
pageCount <- ceiling(recCount$totalNumRecords/1000)
for (i in 1:pageCount){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=1000&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*1000, sep ="")
dataPull <- fromJSON(pageUrl)
}
for (i in 1:1){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=1000&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*1000, sep ="")
dataPull <- fromJSON(pageUrl)
}
for (i in 1:1){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*1000, sep ="")
dataPull <- fromJSON(pageUrl)
}
dataPull$documents$commentText[1]
dataPull$documents$commentText[5]
str(dataPull)
test <- data.frame(dataPull$documents)
View(test)
pageCount <- ceiling(recCount$totalNumRecords/100)
recCount$totalNumRecords
for (i in 1:1){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i=1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comment, tmp)
}
}
for (i in 1:pageCount){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i=1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comment, tmp)
}
}
for (i in 1:pageCount){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comment, tmp)
}
}
for (i in 1:1){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comment, tmp)
}
}
for (i in 1:pageCount){
pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comments, tmp)
}
}
dfCorpus = Corpus(VectorSource(comments$commentText))
#libraries
library(tm)
mycorpus = Corpus(VectorSource(comments$commentText))
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
View(comments)
View(mycorpus)
mycorpus[["4"]][["content"]]
View(comments)
attachList <- comments[comments$attachmentCount>0]
attachList <- subset(comments, attachmentCount > 0)
View(attachList)
?download.file
testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
library(RCurl)
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
testUrl
library(httr)
test <- HEAD(testUrl)
View(test)
test$content
test$all_headers
test$headers$`content-disposition`
test$headers$content-disposition
tmp <- test$headers$`content-disposition`
file = sub(".=\", """, tmp)
?sub
file = sub(".=\", tmp)
download.file(attachUrl, file, mode="wb")
}
}
library(httr)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
myfiles <- list.files(path = comLoc, pattern = "pdf",  full.names = TRUE) #get list of PDFs
comReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Nonrulemaking-Public Submission")
attReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Attachments")
#Placeholder for loading html files
mycorpus <- Corpus(DirSource(comLoc, pattern = "html"))
myhtmlfiles <- list.files(path = comLoc, pattern = "html",  full.names = TRUE) #get list of PDFs
mycorpus = Corpus(VectorSource(comments$commentText))
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
#html test
webpage <- read_html(myhtmlfiles[1])
test <- html_table(html_nodes(webpage, "table"))
file = sub(".=\", tmp)
download.file(attachUrl, file, mode="wb")
}
}
library(httr)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
myfiles <- list.files(path = comLoc, pattern = "pdf",  full.names = TRUE) #get list of PDFs
comReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Nonrulemaking-Public Submission")
attReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Attachments")
#Placeholder for loading html files
mycorpus <- Corpus(DirSource(comLoc, pattern = "html"))
file = substr(tmp,23,length(tmp))
file = substr(tmp,23)
length(tmp)
file = substr(tmp,23,nchar(tmp))
file
file = substr(tmp,23,nchar(tmp)-1)
file
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", document, sep ="")
testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(testUrl)
file <- substr(test$headers$`content-disposition`, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", document, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], " - " , file, sep = ""), mode="wb")
}
}
sum(attachList$attachmentCount)
nrow(attachList)
cnt = 0
?data.frame
?subset
?subset
?data.frame
?colnames
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText), row.names=c("doc_id","text",colnames(subset(comments, select = -c(documentId, commentText)))))
)
View(commentsDf)
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText))))
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText)))
View(commentsDf)
colnames(commentsDf[,1:2]) <- c("doc_id", "text")
View(commentsDf)
colnames(commentsDf[,1]) <- c("doc_id")
colnames(commentsDf[,1]) <- "doc_id"
colnames(commentsDf)[1] <- "doc_id"
View(commentsDf)
colnames(commentsDf)[1:2] <- c("doc_id","text")
mycorpus = Corpus(DataframeSource(commentsDf))
View(mycorpus)
########################
# Load FDMS Reports    #
########################
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
library(janeaustenr)
install.packages("janeaustenr", lib="C:/R/Packages")
library(janeaustenr)
library(dplyr)
library(stringr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
View(original_books)
head(original_books)
library(tidytext)
install.packages("tidytext", lib="C:/R/Packages")
library(tidytext)
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
View(tidy_books)
?unnest_tokens
austen_books()
original_books <- austen_books()
View(original_books)
test <- commentsDf %>%
unnest_tokens(word, text)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
View(commentsDf)
test <- commentsDf %>%
group_by(doc_id) %>%
ungroup()
View(test)
test <- commentsDf %>%
group_by(doc_id) %>%
mutate(linenumber = row_number()) %>%
ungroup()
test1 <- test %>%
unnest_tokens(word, text)
str(test)
str(original_books)
str(comments)
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""), comments$commentText, stringsAsFactors = FALSE)
colnames(commentsDf) <- c("doc_id","text")
mycorpus = Corpus(DataframeSource(commentsDf))
test1 <- test %>%
unnest_tokens(word, text)
test <- commentsDf %>%
group_by(doc_id) %>%
mutate(linenumber = row_number()) %>%
ungroup()
test1 <- test %>%
unnest_tokens(word, text)
View(test1)
data(stop_words)
test1 <- test1 %>%
anti_join(stop_words)
test1 %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE)
library(dplyr)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
dplyr::count(word, sort = TRUE)
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
test1 %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(gutenbergr)
install.packages("gutenbergr", lib="C:/R/Packages")
library(gutenbergr)
gutenberg_metadata
tmp <- gutenberg_metadata
View(tmp)
dumas <- subset(tmp, author="Dumas, Alexandre")
dumas <- subset(tmp, author=="Dumas, Alexandre")
View(dumas)
?subset
dumas <- subset(tmp, author=="Dumas, Alexandre", language=="en")
dumas <- subset(tmp, author=="Dumas, Alexandre" & language=="en")
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
test1 %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
myfiles <- list.files(path = "files/", pattern = "pdf",  full.names = TRUE) #get list of PDFs
attachList <- subset(comments, attachmentCount > 0)
cnt = 0
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], "-", doc, " - " , file, sep = ""), mode="wb")
cnt=cnt+1
}
}
View(attachList)
?write.csv
write.csv(attachList, file="test.csv")
nrow(attachList)
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=CMS-2017-0163-1203&disposition=attachment&attachmentNumber=1, sep ="")
)
attachUrl = "https://www.regulations.gov/contentStreamer?documentId=CMS-2017-0163-1203&disposition=attachment&attachmentNumber=1"
test <- HEAD(attachUrl)
View(test)
test <- HEAD(attachUrl)
View(test)
cnt = 0
?data.frame
errflag = 1
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
if (test$status_code==200){
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], "-", doc, " - " , file, sep = ""), mode="wb")
} else if (errflag==1){
errorLog = data.frame(docId = attachList[comment, "documentId"], error = test$status_code)
errflag = errflag+1
} else {
errorLog[errflag, 1] = attachList[comment, "documentId"]
errorLog[errflag, 2] = error = test$status_code
errflag = errflag+1
}
}
}
View(errorLog)
get_sentiments("afinn")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
test_sentiment <- test1 %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(index, sentiment)) +
geom_col(show.legend = FALSE) +
library(gutenbergr)
ggplot(jane_austen_sentiment, aes(index, sentiment)) +
geom_col(show.legend = FALSE)
bing_word_counts <- test1 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
