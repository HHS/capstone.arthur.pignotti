top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
View(model_top_terms)
write.csv(model_top_terms, file="Data/modelingtopterms.csv")
View(baseline)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart,
Paragraph.Number !between(626, 695)) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart,
Paragraph.Number !between(626, 695)) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
# Remove non-alpha-numeric characters
baselineDoc$Text <- gsub("[^A-z0-9 ]", "", baselineDoc$Text)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart,
Paragraph.Number !between(626, 695)) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baseline <- baseline %>%
filter(Page.Number < 626, Page.Number > 695)
baseline <- baseline %>%
filter(Paragraph.Number < 626, Paragraph.Number > 695)
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total) %>%
filter(total > 5)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
View(base_tf_idf)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baseline <- baseline %>%
filter(Paragraph.Number < 626 || Paragraph.Number > 695)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baseline <- baseline %>%
filter(Paragraph.Number < 626 || Paragraph.Number > 695)
baseline <- baseline %>%
filter(Paragraph.Number < 626 | Paragraph.Number > 695)
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total) %>%
filter(total > 5)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
base_lda <- LDA(base_dtm,
k = length(unique(base$section)),
control = list(seed = 1234))
base_topics <- tidy(base_lda, matrix = "beta")
base_doc_topics <- tidy(base_lda, matrix = "gamma")
test <- base_doc_topics %>%
group_by(document) %>%
filter(gamma == max(gamma))
count <- test %>%
filter(gamma < .75) %>%
arrange(desc(gamma))
View(count)
write.csv(base_doc_topics, file="modelingtest.csv")
multi <- base_doc_topics %>%
filter(gamma > .001) %>%
group_by(topic) %>%
count()
View(multi)
length(unique(base$section)
)
multi <- base_doc_topics %>%
filter(gamma > .001) %>%
group_by(topic) %>%
count() %>%
filter(n > 1)
View(multi)
test1 <- base_topic_docs %>%
group_by(topic) %>%
filter(gamma == max(gamma))
test1 <- base_doc_topics %>%
group_by(topic) %>%
filter(gamma == max(gamma))
View(test1)
write.csv(test1, file="modelingtesttopics.csv")
save(base_lda, file = "Models/lda_test.rda")
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
#Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
#Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
#### Create Corpus Using Tidy ####
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www"))))
#### TF-IDF Testing ####
word_count <- comment.words %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
word_total <- word_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
words <- left_join(word_count, word_total)
words_tf_idf <- words %>%
bind_tf_idf(word, Document.ID, n)
words_tf_idf %>%
filter(total > 8000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
library(tm)
words_dtm <- cast_dtm(words_tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm.topics <- posterior(base_lda, words_dtm)
View(words_dtm)
words_dtm[["dimnames"]][["Docs"]]
?cast_dtm
View(word_count)
words_dtm <- cast_dtm(word_count, document = Document.ID, term = word, count = n)
words_dtm <- cast_dtm(word_count, document = Document.ID, term = word, value = n)
words_dtm.topics <- posterior(base_lda, words_dtm)
View(words_dtm.topics)
new_doc_topics <- tidy(words_dtm.topics, matrix = "gamma")
test_application <- apply(new_doc_topics$topics, 1, which.max)
test_application <- apply(words_dtm.topics$topics, 1, which.max)
#### Convert to DFM ####
words_dfm <- cast_dfm(word_count, document = Document.ID, term = word, value = n)
#### Convert to DFM ####
library(quanteda)
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
mutate(word = wordStem(word))
#### TF-IDF Testing ####
word_count <- comment.words %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
word_total <- word_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
words <- left_join(word_count, word_total)
words_tf_idf <- words %>%
bind_tf_idf(word, Document.ID, n)
words_dtm <- cast_dtm(word_count, document = Document.ID, term = word, value = n)
words_dfm <- cast_dfm(word_count, document = Document.ID, term = word, value = n)
?textstat_simil
text <- textstat_simil(words_dfm)
text_sim <- textstat_simil(words_dfm)
text_sim
simdf <- as.data.frame(text_sim)
text_sim <- textstat_simil(words_dfm, margin = "documents", method = "cosine")
m <- data.frame(t(combn(rownames(text_sim),2)), as.numeric(d))
m <- data.frame(t(combn(rownames(text_sim),2)), as.numeric(text_sim))
m <- as.matrix(text_sim)
View(m)
m <- as.data.frame(as.matrix(text_sim))
m2 <- melt(m)[melt(upper.tri(m))$value,]
library(reshape)
install.packages("reshape")
library(reshape)
m2 <- melt(m)[melt(upper.tri(m))$value,]
names(m2) <- c("c1", "c2", "distance")
write.csv(m2, file = "Data/similarity.csv")
test <- as.data.frame(m2)
head(test)
words_dtm.topics$topics[1]
words_dtm.topics$topics
words_dtm.topics$topics[[1]]
words_dtm.topics$topics[1]
words_dtm.topics[1]$topics
words_dtm.topics$topics[50]
test <- as.data.frame(words_dtm.topics$topics)
View(test)
test <- as.data.frame(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)
View(test)
rownames(words_dtm.topics$topics)
test <- cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)
View(test)
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics))
View(test)
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics),
row.names = FALSE)
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics))
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = Document.ID, value = value)
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:80)
View(test)
test <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics))
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:80)
View(words_scoring)
simdf <- as.data.frame(text_sim)
text_sim
View(m2)
require(vegan)
install.packages("vegan")
simdf <- as.data.frame(text_sim)
inspect(text_sim)
summary(text_sim)
str(text_sim)
View(m)
words_sim <- as.data.frame(cbind(Document.ID = rownames(as.matrix(text_sim)),
as.matrix(text_sim))) %>%
install.packages("reshape")
words_sim <- as.data.frame(cbind(Document.ID = rownames(as.matrix(text_sim)),
as.matrix(text_sim)))
View(words_sim)
text_sim <- textstat_simil(words_dfm, margin = "documents", method = "cosine", upper = FALSE, diag = FALSE)
text_sim[1]
text_sim$Labels[1]
text_sim$Labels
data.frame( t(combn(names(words_sim),2)), dist=t(words_sim)[lower.tri(words_sim)] )
test <- data.frame( t(combn(names(words_sim),2)), dist=t(words_sim)[lower.tri(words_sim)] )
subset(melt(words_sim), value!=1)
words_sim <- as.data.frame( as.matrix(text_sim)))
words_sim <- as.data.frame( as.matrix(text_sim))
subset(melt(words_sim), value!=1)
test <- subset(melt(words_sim), value!=1)
head(test)
#### Create Topic Map ####
topic_map1 <- base_doc_topics %>%
group_by(document) %>%
filter(gamma == max(gamma))
topic_map2 <- base_doc_topics %>%
group_by(topic) %>%
filter(gamma == max(gamma))
topic_map <- rbind(topic_map1, topic_map2) %>%
distinct()
View(topic_map)
topic_map <- rbind(topic_map1, topic_map2) %>%
distinct() %>%
select(-gamma)
topic_map <- rbind(topic_map1, topic_map2) %>%
distinct() %>%
select(-gamma)
View(topic_map)
save(base_lda, file = "Models/topic_map.csv")
write.csv(base_lda, file = "Models/topic_map.csv", row.names = FALSE)
write.csv(base_lda, file = "Models/topic_map.csv", row.names = FALSE)
write.csv(topic_map, file = "Models/topic_map.csv", row.names = FALSE)
topic_map <- read.csv(file = "Models/topic_map.csv")
View(topic_map)
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:80) %>%
right_join(topic_map, by = c("Topic" = "topic"))
str(topic_map)
str(words_scoring)
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:80) %>%
mutate(Topic = as.numeric(Topic)) %>%
right_join(topic_map, by = c("Topic" = "topic"))
str(words_scoring)
View(words_scoring)
write.csv(words_scoring, file = "Data/scoring.csv", row.names = FALSE)
#####################
# Initial Setup     #
#####################
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
#libraries
library(tm)
library(readxl)
library(rvest)
library(scrapeR)
library(jsonlite)
library(httr)
library(tidyverse)
#####################
# API Data Call     #
#####################
dktid = "CMS-2017-0163"
api_key = "bbPnmY2FqvoazRuHseN0liEsWh0qI255CgJTsPAo"
#API call to get the number of record calls needs. API limits pull to 1000 records
countUrl = paste0("https://api.data.gov:443/regulations/v3/documents.json?api_key=",api_key,"&countsOnly=1&encoded=1&dct=PS&dktid=", dktid)
recCount <- fromJSON(countUrl)
pageCount <- ceiling(recCount$totalNumRecords/100)
for (i in 1:pageCount){
pageUrl = paste0("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100)
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comments, tmp)
}
}
write.csv(comments, file = "Data/comments.csv", row.names = FALSE)
########################
# Download Attachments #
########################
attachList <- subset(comments, attachmentCount > 0)
errflag = 1
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
if (test$status_code==200){
tmp <- test$headers$`content-disposition`
file <- substr(tmp, nchar(tmp)-1, nchar(tmp)-1)
file <- str_extract(tmp, "\\.[A-Za-z]{3,4}")
download.file(attachUrl, paste0("files/",  attachList[comment, "documentId"], "-", doc, file), mode="wb")
} else if (errflag==1){
errorLog = data.frame(docId = attachList[comment, "documentId"], error = test$status_code)
errflag = errflag+1
} else {
errorLog[errflag, 1] = attachList[comment, "documentId"]
errorLog[errflag, 2] = error = test$status_code
errflag = errflag+1
}
}
}
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
dktid = "CMS-2017-0163"
#libraries
library(tm)
library(readxl)
library(tidyverse)
source("helper_functions.R")
#### Load Raw Datasets ####
comLoc <- "C:/Data/Comments/Testing"
#Comment Report from FDMS
comReport <- read_excel(paste0(comLoc, "/report.xlsx"), sheet = 1)
colnames(comReport) <- make.names(colnames(comReport))
#Comment Report from FDMS
attReport <- read_excel(paste0(comLoc, "/report.xlsx"), sheet = 2)
colnames(attReport) <- make.names(colnames(attReport))
#Comment Dataset from Data_Download.R
comments <- read_csv("Data/comments.csv")
#Map of Comments, Commenters, Commenter Categories
map <- read_excel(paste0(comLoc, "/map.xlsx"))
colnames(map) <- make.names(colnames(map))
#Text Extract of Comment Attachments
attachExtract <- read_excel("Data/CMS-2017-0163-TExtract.xlsx")
colnames(attachExtract) <- make.names(colnames(attachExtract))
comReport$Site_Key <- substring(comReport$Email.Address, regexpr("@", comReport$Email.Address) + 1)
#### Load Text Extract ####
attachExtract <- attachExtract %>%
mutate(Comment.ID = substr(File.Name,1,18),
Document.ID = substr(File.Name, 1, regexpr("\\.", File.Name) - 1)) %>%
select(Comment.ID, Document.ID, Text, Page.Number) %>%
left_join(comReport, by = c("Comment.ID" = "Document.ID")) %>%
left_join(map, by = c("Comment.ID" = "Document"))
commentsDf <- comments %>%
mutate(Document.ID = paste0(documentId, "-0"),
Comment.ID = documentId,
Text = commentText,
Page.Number = 1) %>%
select(Comment.ID, Document.ID, Text, Page.Number) %>%
left_join(comReport, by = c("Comment.ID" = "Document.ID")) %>%
left_join(map, by = c("Comment.ID" = "Document")) %>%
union(attachExtract)
#### Filter Out Message Comments Only Referencing Attachments Comments ####
# Create test file to review and find word count cutoff
testComment <- commentsDf %>%
filter(!(word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")))
write.csv(testComment, file = "Data/testAttacted.csv")
# Apply word filter with word count cutoff
testComment <- commentsDf %>%
filter(!((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")) & wordcount(Text) < 150))
#### Export Cleaned File for Text Mining
write.csv(commentsDf, file = "Data/commentsDf.csv", row.names = FALSE)
View(commentsDf)
#Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
mutate(word = wordStem(word))
#### TF-IDF Testing ####
word_count <- comment.words %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
word_total <- word_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
words <- left_join(word_count, word_total)
words_tf_idf <- words %>%
bind_tf_idf(word, Document.ID, n)
words_tf_idf %>%
filter(total > 8000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
library(tm)
words_dtm_tf_idf <- cast_dtm(words_tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm <- cast_dtm(word_count, document = Document.ID, term = word, value = n)
#### Apply Base Model ####
library(topicmodels)
load("Models/lda_test.rda")
topic_map <- read.csv(file = "Models/topic_map.csv")
words_dtm.topics <- posterior(base_lda, words_dtm)
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:80) %>%
mutate(Topic = as.numeric(Topic)) %>%
right_join(topic_map, by = c("Topic" = "topic"))
write.csv(words_scoring, file = "Data/scoring.csv", row.names = FALSE)
View(words_dtm.topics)
