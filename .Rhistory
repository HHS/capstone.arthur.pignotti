rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index, by = c("index" = "index1")) %>%
anti_join(bigram.index, by = c("index" = "index2")) %>%
mutate(word = wordStem(word))
#write.csv(baseline.unigram, file = "unigram_index.csv", row.names = FALSE)
baseline <- union_all(baseline.unigram, bigram.index) %>%
select(-c(index1, index2)) %>%
arrange(Paragraph.Number)
write.csv(baseline, file = "baseline.csv", row.names = FALSE)
baseline.count <- baseline %>%
count(section, word, sort = TRUE)
baseline.total <- baseline.count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(baseline.count, baseline.total)
baseline.tf_idf <- base %>%
bind_tf_idf(word, section, n)
baseline.dtm <- cast_dtm(baseline.tf_idf, section, word, n)
#### Train topic model ####
base_lda <- LDA(baseline.dtm,
k = 90,
control = list(seed = 1234))
save(base_lda, file = "Models/lda_test.rda")
model.topic.term <- tidy(base_lda, matrix = "beta")
model.doc.topic <- tidy(base_lda, matrix = "gamma")
model.doc.term <- inner_join(model.topic.term, model.doc.topic, by = c("topic" = "topic")) %>%
mutate(score = beta * gamma) %>%
select(document, term, score, gamma) %>%
group_by(document, term) %>%
summarise(score = sum(score)/sum(gamma)) %>%
arrange(desc(score))
write.csv(model.doc.term, file="Models/modelingTopicsTermsBeta.csv", row.names = FALSE)
write.csv(model.doc.topic, file="Models/modelingDocsTopicsGamma.csv", row.names = FALSE)
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta*gamma)/sum(gamma)) %>%
ungroup() %>%
group_by(document) %>%
top_n(200, final_beta) %>%
ungroup()
write.csv(doc.topic.terms, file = "termTesting.csv", row.names = FALSE)
#### Create Word Clouds of Basline Sections ####
library(wordcloud)
library(RColorBrewer)
pal2 <- brewer.pal(8,"Dark2")
for (i in 1:length(unique(doc.topic.terms$document))) {
doc <- unique(doc.topic.terms$document)[i]
subset <- doc.topic.terms %>%
filter(document == doc)
png(paste0("clouds/", doc,".png"), width=1280,height=800)
wordcloud(subset$term,
subset$final_beta,
scale=c(8,.3),
colors=pal2)
dev.off()
}
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) # add library location - library workaround
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") # local location of github repo
source("helper_functions.R") # load helper functions
# Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(quanteda)
library(topicmodels)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv",
stringsAsFactors = FALSE) # dataset of comments
bigram.list <- read.csv("Data/bigram_list2.csv",
stringsAsFactors = FALSE) # load list of relevant bigrams
#### Create Corpus Using Tidy ####
data(stop_words) # load common English stop words
cms_stop <- read_csv("Data/cms_stop_words.csv") # load CMS-specific stop words
# Remove non-alpha-numeric characters
#commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#commentsDf <- mutate(commentsDf, Text = str_replace_all(Text, "-", " "))
## Unnest tokens, integrate bigrams, and remove stop words ##
# Unnnest bigram tokens
bigram.index <- commentsDf %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2) %>% # unnest bigram tokens
rowid_to_column("index") %>% # add index column
separate(bigram,
c("word1", "word2"),
sep = " ") %>% # separate bigram into two words for stemming
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) # stem separated bigrams
# Fix bigram unnesting so it matches word unnesting. Bigram unnesting will not unnest the final word of a cell into the first word of the next cell or by itself so the indexing between bigrams and words will not naturally match. So, we are unnesting the second word from the final bigram from each Document.ID-Page.Number combo to create a separate row with the final word by itself
bigram.last.row <- bigram.index %>%
group_by(Document.ID,
Page.Number) %>% # group by Document.ID and Page.Number
filter(row_number() == n()) %>% # filters for the last row of each group
ungroup() %>%
mutate(word = word2, # transform the bigram into the final word
index = index + 1) %>% # increment the index so it arrange at the end of the Document.ID-Page.Number group when reintegrated
filter(!is.na(word)) %>% # remove NA that show up when unnesting text with less than two words
select(-c(word1, word2)) #
bigram.index <- bigram.index %>%
unite(word,
word1,
word2,
sep = " ") %>% # recombine stemmed bigrams
union_all(bigram.last.row) %>% # combine the original unnested bigram and the single word fix
arrange(Document.ID,
Page.Number,
index) %>% # re
rowid_to_column("index1") %>%
select(-index) %>%
inner_join(bigram.list, by = c("word" = "word")) %>%
mutate(index2 = index1 + 1, index = index1)
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index, by = c("index" = "index1")) %>%
anti_join(bigram.index, by = c("index" = "index2")) %>%
mutate(word = wordStem(word))
comments <- union_all(comment.words, bigram.index) %>%
select(-c(index1, index2))
#### TF-IDF Testing ####
words_dtm_tf_idf <- cast_dtm(comment.tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
comment.count <- comments %>%
mutate(Document.ID = paste0(Document.ID,"-",Page.Number)) %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count, comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word, Document.ID, n)
comment.tf_idf %>%
filter(total > 1000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
#### Apply Base Model ####
# Load model created in Setup_Baseline_for_Model.R
load("Models/lda_test.rda")
topic_map <- read.csv(file = "Models/topic_map.csv")
model.doc.term <- read.csv(file="Models/modelingTopicsTermsBeta.csv")
model.doc.topic <- read.csv(file="Models/modelingDocsTopicsGamma.csv")
# Run model against comments
words_dtm.topics <- posterior(base_lda, words_dtm)
# Transform results into tidy dataframe and map scores to document sections
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:91) %>%
mutate(Topic = as.numeric(Topic)) %>%
right_join(model.doc.topic, by = c("Topic" = "topic")) %>%
mutate(final_score = as.numeric(Score) * gamma) %>%
group_by(Document.ID, document) %>%
summarise(sum = sum(final_score)/sum(gamma)) %>%
arrange(-sum) %>%
ungroup()
# Calculate min and max score of each section to normalize scores
minmax.base <- words_scoring %>%
group_by(document) %>%
summarise(min = min(sum, na.rm = TRUE),
max = max(sum, na.rm = TRUE))
# Mix-Max normalize scores
words_scoring.norm <- words_scoring %>%
inner_join(minmax.base, by = c("document" = "document")) %>%
mutate(score = (sum-min)/(max-min))
# Transform dataframe for final use
words_scoring.matrix <- words_scoring.norm %>%
mutate(score = if_else(score < .001, 0, score)) %>% # Scores below .1% are changed to zero to remove scientific notation
select(-c(sum, min, max)) %>% # Remove unneeded scoring components
spread(document, score) # Transform tidy dataframe into wide format
# Write out results
write.csv(words_scoring.matrix, file = "Data/results.csv", row.names = FALSE)
# Create histograms of scores for each section to look at distribution
for (i in 1:length(unique(words_scoring.norm$document))) {
doc <- unique(words_scoring.norm$document)[i]
subset <- words_scoring.norm %>%
filter(document == doc) %>%
filter(score > .01)
png(paste0("score_hists/", doc,".png"), width=1280,height=800)
hist(subset$score, main = doc)
dev.off()
}
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
words_dtm_tf_idf <- cast_dtm(comment.tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
topic_map <- read.csv(file = "Models/topic_map.csv")
model.doc.term <- read.csv(file="Models/modelingTopicsTermsBeta.csv")
model.doc.topic <- read.csv(file="Models/modelingDocsTopicsGamma.csv")
# Run model against comments
words_dtm.topics <- posterior(base_lda, words_dtm)
# Transform results into tidy dataframe and map scores to document sections
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:91) %>%
mutate(Topic = as.numeric(Topic)) %>%
right_join(model.doc.topic, by = c("Topic" = "topic")) %>%
mutate(final_score = as.numeric(Score) * gamma) %>%
group_by(Document.ID, document) %>%
summarise(sum = sum(final_score)/sum(gamma)) %>%
arrange(-sum) %>%
ungroup()
# Calculate min and max score of each section to normalize scores
minmax.base <- words_scoring %>%
group_by(document) %>%
summarise(min = min(sum, na.rm = TRUE),
max = max(sum, na.rm = TRUE))
# Mix-Max normalize scores
words_scoring.norm <- words_scoring %>%
inner_join(minmax.base, by = c("document" = "document")) %>%
mutate(score = (sum-min)/(max-min))
# Transform dataframe for final use
words_scoring.matrix <- words_scoring.norm %>%
mutate(score = if_else(score < .001, 0, score)) %>% # Scores below .1% are changed to zero to remove scientific notation
select(-c(sum, min, max)) %>% # Remove unneeded scoring components
spread(document, score) # Transform tidy dataframe into wide format
# Write out results
write.csv(words_scoring.matrix, file = "Data/results.csv", row.names = FALSE)
# Create histograms of scores for each section to look at distribution
for (i in 1:length(unique(words_scoring.norm$document))) {
doc <- unique(words_scoring.norm$document)[i]
subset <- words_scoring.norm %>%
filter(document == doc) %>%
filter(score > .01)
png(paste0("score_hists/", doc,".png"), width=1280,height=800)
hist(subset$score, main = doc)
dev.off()
}
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) # add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") # local location of github repo
source("helper_functions.R")
# Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(topicmodels)
library(quanteda)
library(visNetwork)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
bigram.list <- read.csv("Data/bigram_list2.csv", stringsAsFactors = FALSE)
#### Create Corpus Using Tidy ####
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#Unnest tokens and remove stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
#### Create TF-IDF ####
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
baseline <- read_csv("baseline.csv")
View(baseline)
View(commentsDf)
baselineDoc <- read_excel("Data/AN_Part2 - V4.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
View(baselineDoc)
#Unnest tokens and remove stop words
docs <- commentsDf %>%
select(Document.ID, Text) %>%
union_all(select(baselineDoc,
Document.ID,
Text))
comment.words <- docs %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baselineName <- unique(baselineDoc$Document.ID)
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
words_dfm <- cast_dfm(comment.count,
document = Document.ID,
term = word,
value = n)
words_dfm_tf_idf <- cast_dfm(comment.tf_idf,
document = Document.ID,
term = word,
value = tf_idf)
words_dtm <- cast_dtm(comment.tf_idf,
Document.ID,
word,
n)
# Calculate similarity matrix and tidy it up
similarity <- textstat_simil(words_dfm,
margin = "documents",
method = "cosine") %>%
as.dist() %>%
tidy() %>%
arrange(desc(distance))
names(similarity) <- c("doc1", "doc2", "distance")
head(similarity)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc1 == baselineName)
head(baseSim)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName)
head(baseSim)
hist(baseSim$distance)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName) %>%
arrange(-distance)
head(baseSim)
similarity <- similarity %>%
filter(!(doc1 == baselineName | doc2 == baselineName))
?rename
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc1 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc2 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim <- union_all(baseSim1,
baseSim2)
View(baseSim)
unique(baseSim$doc1)
baseSim <- baseSim1 %>%
union_all(baseSim2) %>%
arrange(-distance)
View(baseSim)
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) # add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") # local location of github repo
source("helper_functions.R")
# Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(topicmodels)
library(quanteda)
library(visNetwork)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
bigram.list <- read.csv("Data/bigram_list2.csv", stringsAsFactors = FALSE)
baselineDoc <- read_excel("Data/AN_Part2 - V4.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
#### Create Corpus Using Tidy ####
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#Unnest tokens and remove stop words
docs <- commentsDf %>%
select(Document.ID, Text) %>%
union_all(select(baselineDoc,
Document.ID,
Text))
baselineName <- unique(baselineDoc$Document.ID)
comment.words <- docs %>%
unnest_tokens(word, Text) %>%
#    filter(!(str_detect(word, regex("^http"))),
#           !(str_detect(word, regex("^www")))) %>%
#    anti_join(stop_words) %>%
#    anti_join(cms_stop) %>%
#    mutate(word = wordStem(word))
#### Create TF-IDF ####
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
#### Convert to DFM and Calculate Similarity Matrices ####
words_dfm <- cast_dfm(comment.count,
document = Document.ID,
term = word,
value = n)
words_dfm_tf_idf <- cast_dfm(comment.tf_idf,
document = Document.ID,
term = word,
value = tf_idf)
words_dtm <- cast_dtm(comment.tf_idf,
Document.ID,
word,
n)
# Calculate similarity matrix and tidy it up
similarity <- textstat_simil(words_dfm,
margin = "documents",
method = "cosine") %>%
as.dist() %>%
tidy() %>%
arrange(desc(distance))
names(similarity) <- c("doc1", "doc2", "distance")
head(similarity)
#### Separate out baseline similarities ####
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName) %>%
arrange(-distance)
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc2 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim <- baseSim1 %>%
union_all(baseSim2) %>%
arrange(-distance)
unique(baseSim$doc1)
similarity <- similarity %>%
filter(!(doc1 == baselineName | doc2 == baselineName))
View(baseSim)
write.csv(baseSim,
file = "Data/Similarity_to_Baseline.csv",
row.names = FALSE)
# Similarity to Baseline Document Threshold
baseSimThres = .95
simRemoves <- baseSim %>%
filter(distance > baseSimThres) %>%
select(doc2) %>%
rename(doc2 = doc)
simRemoves <- baseSim %>%
filter(distance > baseSimThres) %>%
select(doc2) %>%
rename(doc = doc2)
View(simRemoves)
filter(!(Document.ID %in% simRemoves$doc)
commentsDf <- commentsDf %>%
commentsDf <- commentsDf %>%
)
commentsDf <- commentsDf %>%
filter(!(Document.ID %in% simRemoves$doc))
View(commentsDf)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv",
stringsAsFactors = FALSE) # dataset of comments
test <- commentsDf %>%
filter((Document.ID %in% simRemoves$doc))
commentsDf <- commentsDf %>%
filter(!(Document.ID %in% simRemoves$doc))
#### Investigate similar comments ####
hist(filter(similarity, distance > .85)$distance)
