.libPaths( c( .libPaths(), "~/userLibrary") )
.libPaths( c( .libPaths(), "C:/R/Packages") )
.libPaths
.libPaths()
install.packages("dplyr", lib="C:/R/Packages")
.libPaths()
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("ggplot")
install.packages("ggplot", lib="C:/R/Packages")
install.packages("ggplot2")
install.packages("caret")
install.packages("sqldf")
install.packages("devtools")
install.packages("jsonlite")
install.packages("pdftools")
library(pdftools)
library("pdftools", lib.loc="C:/R/Packages")
library("ggplot2", lib.loc="C:/R/Packages")
library("codetools", lib.loc="C:/Program Files/R/R-3.3.3/library")
library("colorspace", lib.loc="C:/R/Packages")
library("crayon", lib.loc="C:/R/Packages")
.libPaths()
install.packages("ggplot2")
library(ggplot2)
install.packages("Rcpp")
install.packages("Rcpp")
library(ggplot2)
library("pdftools", lib.loc="C:/R/Packages")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[[1]], "\n")[[1]])
install.packages("tesseract")
library(tesseract)
eng <- tesseract("eng")
text <- tesseract::ocr("http://jeroen.github.io/images/testocr.png", engine = eng)
text <- tesseract::ocr("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf", engine = eng)
text <- tesseract::ocr("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
results <- tesseract::ocr_data("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
print(text, n = 20)
print(results, n = 20)
View(results)
head(results.confidence)
summary(results$confidence)
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("shiny", lib="C:/R/Packages")
install.packages("yhat", lib="C:/R/Packages")
install.packages("stringr", lib="C:/R/Packages")
install.packages("forecast", lib="C:/R/Packages")
install.packages("randomForest", lib="C:/R/Packages")
install.packages("reshape2", lib="C:/R/Packages")
install.packages("lubridate", lib="C:/R/Packages")
install.packages("data.table", lib="C:/R/Packages")
install.packages("googleVis", lib="C:/R/Packages")
install.packages("caret", lib="C:/R/Packages")
install.packages("car", lib="C:/R/Packages")
install.packages("tm", lib="C:/R/Packages")
install.packages("twitteR", lib="C:/R/Packages")
install.packages("gbm", lib="C:/R/Packages")
install.packages("e1071", lib="C:/R/Packages")
install.packages("glmnet", lib="C:/R/Packages")
install.packages("tau", lib="C:/R/Packages")
install.packages("knitr", lib="C:/R/Packages")
install.packages("NLP", lib="C:/R/Packages")
.libPaths( c("C:/R/Packages", .libPaths()) )
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tidyverse)
########################
# Load FDMS Reports    #
########################
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
########################
# Load FDMS Reports    #
########################
library(readxl)
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
warnings()
unique(comReport)
count(comReport)
test <- unique(comReport)
View(test)
test <- count(comReport)
View(test)
test <- unique(comReport$Attachments)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
str(comReport)
test <- unique(comReport$`Organization Name`)
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
View(comReport)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))["Site_Key"]
siteDf <- distinct(siteDf)
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name"]
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
View(siteDf)
siteDf <- distinct(siteDf)
########################
# Load FDMS Reports    #
########################
library(tidyverse)
.libPaths( c("C:/R/Packages", .libPaths()) )
########################
# Load FDMS Reports    #
########################
library(tidyverse)
siteDf <- distinct(siteDf)
########################
# Web Scraping         #
########################
library(rvest)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
webpage %>%
html_node("body") %>%
html_text()
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[5, 'Site_Key'], sep='')
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
library(httr)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'me.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
siteDf <- distinct(siteDf)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
test <- HEAD(url)
test <- http_error(url)
test <- url_success(url)
test <- !http_error(url)
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tid)
library(tidyverse)
tidyverse_update()
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("stm", lib="C:/R/Packages")
install.packages("quanteda", lib="C:/R/Packages")
#####################
# Initial Setup     #
#####################
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
#Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
data(stop_words)
test <- commentsDf %>%
group_by(Document.ID) %>%
mutate(linenumber = row_number()) %>%
ungroup()
View(test)
comment.words <- commentsDf %>%
unnest_tokens(word, Text)
cms_stop <- data.frame(word = c("cms","medicare","ma", "plan", "care", "beneficiaries", "advantage", "proposed", "rule", "health", "plans", "md", "baltimore", "seema", "verma", "washington", "dc", "boulevardbaltimore"), stringsAsFactors = FALSE)
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop)
write.csv(comment.words, file = "Data/commentWords.csv", row.names = FALSE)
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www"))))
write.csv(comment.words, file = "Data/commentWords.csv", row.names = FALSE)
#######################
# TF-IDF Testing      #
#######################
word_count <- comment.words %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
spell.find <- hunspell_find(comment.words$word)
library(hunspell)
spell.find <- hunspell_find(comment.words$word)
spell.count <- as.data.frame(matrix(unlist(spell.find)),
byrow = TRUE,
stringsAsFactors = FALSE)
spell.count.unique <- as.data.frame(matrix(unique(unlist(spell.find)),
byrow = TRUE),
stringsAsFactors = FALSE)
spell.suggest <- hunspell_suggest(spell.count.unique$V1)
#Function to find the most amount of elements in a list
elementMax <- function(list){
maxEl = 0
for (i in 1:length(list)){
if (length(list[[i]]) > maxEl) {
maxEl <- length(list[[i]])
}
}
return(maxEl)
}
#Create blank data.frame to input spelling suggestions into
spell.suggest.df <- data.frame(word = matrix(unlist(spell.count.unique), byrow = TRUE),
matrix(nrow = length(spell.suggest),
ncol = elementMax(spell.suggest)))
#Put spelling suggestions into a data frame
for (i in 1:14772){
if (length(spell.suggest[[i]]) != 0){
for (j in 1:length(spell.suggest[[i]])){
spell.suggest.df[i, j+1] = spell.suggest[[i]][j]
}
}
}
#Create blank data.frame to input spelling find results into
spell.find.df <- data.frame(Document.ID = comment.words$Document.ID,
word = comment.words$word,
misspell = matrix(nrow = dim(comment.words)[1], ncol = 1))
#Put spelling find results into a data frame
for (i in 1:length(spell.find)){
if (length(spell.find[[i]]) != 0){
spell.find.df[i, 3] = spell.find[[i]][1]
}
}
spell.count.overall <- spell.find.df %>%
group_by(Document.ID) %>%
count()
spell.count.find <- spell.find.df %>%
filter(!is.na(misspell)) %>%
group_by(Document.ID) %>%
count()
spell.count.overall <- spell.count.overall %>%
left_join(spell.count.find, by = c("Document.ID" = "Document.ID"))
spell.count.overall$n.y <- spell.count.overall$n.y %>%
replace_na(0)
spell.count.overall <- spell.count.overall %>%
mutate(percent.misspelled = n.y/n.x) %>%
arrange(desc(percent.misspelled))
spell.count.overall %>%
ggplot(aes(x = percent.misspelled)) +
geom_density()
spell.count.overall %>%
filter(n.x < 2000) %>%
ggplot(aes(x = n.x)) +
geom_density()
median(spell.count.overall$n.x)
mean(spell.count.overall$n.x)
View(spell.count.overall)
?unnest_tokens
#Load CMS-specific stop words
cms_stop <- data.frame(word = c("CMS","Medicare","MA", "plan", "care", "beneficiaries", "Advantage", "proposed", "rule", "health", "plans", "MD", "Baltimore", "Seema", "Verma", "Washington", "DC", "boulevardbaltimore"), stringsAsFactors = FALSE)
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text, to_lower = FALSE) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www"))))
spell.find <- hunspell_find(comment.words$word)
spell.count <- as.data.frame(matrix(unlist(spell.find)),
byrow = TRUE,
stringsAsFactors = FALSE)
spell.count.unique <- as.data.frame(matrix(unique(unlist(spell.find)),
byrow = TRUE),
stringsAsFactors = FALSE)
spell.suggest <- hunspell_suggest(spell.count.unique$V1)
#Function to find the most amount of elements in a list
elementMax <- function(list){
maxEl = 0
for (i in 1:length(list)){
if (length(list[[i]]) > maxEl) {
maxEl <- length(list[[i]])
}
}
return(maxEl)
}
#Create blank data.frame to input spelling suggestions into
spell.suggest.df <- data.frame(word = matrix(unlist(spell.count.unique), byrow = TRUE),
matrix(nrow = length(spell.suggest),
ncol = elementMax(spell.suggest)))
#Put spelling suggestions into a data frame
for (i in 1:14772){
if (length(spell.suggest[[i]]) != 0){
for (j in 1:length(spell.suggest[[i]])){
spell.suggest.df[i, j+1] = spell.suggest[[i]][j]
}
}
}
#Put spelling suggestions into a data frame
for (i in 1:14652){
if (length(spell.suggest[[i]]) != 0){
for (j in 1:length(spell.suggest[[i]])){
spell.suggest.df[i, j+1] = spell.suggest[[i]][j]
}
}
}
#Create blank data.frame to input spelling find results into
spell.find.df <- data.frame(Document.ID = comment.words$Document.ID,
word = comment.words$word,
misspell = matrix(nrow = dim(comment.words)[1], ncol = 1))
#Put spelling find results into a data frame
for (i in 1:length(spell.find)){
if (length(spell.find[[i]]) != 0){
spell.find.df[i, 3] = spell.find[[i]][1]
}
}
spell.count.overall <- spell.find.df %>%
group_by(Document.ID) %>%
count()
spell.count.find <- spell.find.df %>%
filter(!is.na(misspell)) %>%
group_by(Document.ID) %>%
count()
spell.count.overall <- spell.count.overall %>%
left_join(spell.count.find, by = c("Document.ID" = "Document.ID"))
spell.count.overall$n.y <- spell.count.overall$n.y %>%
replace_na(0)
spell.count.overall <- spell.count.overall %>%
mutate(percent.misspelled = n.y/n.x) %>%
arrange(desc(percent.misspelled))
View(spell.count.overall)
write.csv(spell.count.overall,
file = "Data/spellCheckMetric.csv",
row.names = FALSE)
