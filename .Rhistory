.libPaths( c( .libPaths(), "~/userLibrary") )
.libPaths( c( .libPaths(), "C:/R/Packages") )
.libPaths
.libPaths()
install.packages("dplyr", lib="C:/R/Packages")
.libPaths()
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("ggplot")
install.packages("ggplot", lib="C:/R/Packages")
install.packages("ggplot2")
install.packages("caret")
install.packages("sqldf")
install.packages("devtools")
install.packages("jsonlite")
install.packages("pdftools")
library(pdftools)
library("pdftools", lib.loc="C:/R/Packages")
library("ggplot2", lib.loc="C:/R/Packages")
library("codetools", lib.loc="C:/Program Files/R/R-3.3.3/library")
library("colorspace", lib.loc="C:/R/Packages")
library("crayon", lib.loc="C:/R/Packages")
.libPaths()
install.packages("ggplot2")
library(ggplot2)
install.packages("Rcpp")
install.packages("Rcpp")
library(ggplot2)
library("pdftools", lib.loc="C:/R/Packages")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[[1]], "\n")[[1]])
install.packages("tesseract")
library(tesseract)
eng <- tesseract("eng")
text <- tesseract::ocr("http://jeroen.github.io/images/testocr.png", engine = eng)
text <- tesseract::ocr("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf", engine = eng)
text <- tesseract::ocr("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
results <- tesseract::ocr_data("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
print(text, n = 20)
print(results, n = 20)
View(results)
head(results.confidence)
summary(results$confidence)
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("shiny", lib="C:/R/Packages")
install.packages("yhat", lib="C:/R/Packages")
install.packages("stringr", lib="C:/R/Packages")
install.packages("forecast", lib="C:/R/Packages")
install.packages("randomForest", lib="C:/R/Packages")
install.packages("reshape2", lib="C:/R/Packages")
install.packages("lubridate", lib="C:/R/Packages")
install.packages("data.table", lib="C:/R/Packages")
install.packages("googleVis", lib="C:/R/Packages")
install.packages("caret", lib="C:/R/Packages")
install.packages("car", lib="C:/R/Packages")
install.packages("tm", lib="C:/R/Packages")
install.packages("twitteR", lib="C:/R/Packages")
install.packages("gbm", lib="C:/R/Packages")
install.packages("e1071", lib="C:/R/Packages")
install.packages("glmnet", lib="C:/R/Packages")
install.packages("tau", lib="C:/R/Packages")
install.packages("knitr", lib="C:/R/Packages")
install.packages("NLP", lib="C:/R/Packages")
.libPaths( c("C:/R/Packages", .libPaths()) )
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tidyverse)
########################
# Load FDMS Reports    #
########################
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
########################
# Load FDMS Reports    #
########################
library(readxl)
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
warnings()
unique(comReport)
count(comReport)
test <- unique(comReport)
View(test)
test <- count(comReport)
View(test)
test <- unique(comReport$Attachments)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
str(comReport)
test <- unique(comReport$`Organization Name`)
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
View(comReport)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))["Site_Key"]
siteDf <- distinct(siteDf)
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name"]
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
View(siteDf)
siteDf <- distinct(siteDf)
########################
# Load FDMS Reports    #
########################
library(tidyverse)
.libPaths( c("C:/R/Packages", .libPaths()) )
########################
# Load FDMS Reports    #
########################
library(tidyverse)
siteDf <- distinct(siteDf)
########################
# Web Scraping         #
########################
library(rvest)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
webpage %>%
html_node("body") %>%
html_text()
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[5, 'Site_Key'], sep='')
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
library(httr)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'me.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
siteDf <- distinct(siteDf)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
test <- HEAD(url)
test <- http_error(url)
test <- url_success(url)
test <- !http_error(url)
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tid)
library(tidyverse)
tidyverse_update()
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("stm", lib="C:/R/Packages")
install.packages("quanteda", lib="C:/R/Packages")
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
#baseStart = 66
# Load libraries
library(tidyverse)
library(readxl)
library(tidytext)
library(tm)
library(SnowballC)
library(topicmodels)
library(quanteda)
# Load baseline document for building model
baselineDoc <- read_excel("Data/AN - V5.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#baselineDoc$Text <- gsub("[^A-z0-9 ]", " ", baselineDoc$Text)
# Add section column and removes lines paragraphs that are mostly numbers, which are generally parts of tables
baselineDoc_section <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "") %>%
#filter(Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, ifelse(str_detect(Text, regex("^attachment [A-Z]", ignore_case = TRUE)), Text, NA))) %>%
mutate(Text = str_replace_all(Text, "-", " ")) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
filter(str_detect(section, regex("^section [A-Z]", ignore_case = TRUE)))
# Remove table of contents for call letter
baselineDoc_section <- baselineDoc_section %>%
filter(Paragraph.Number < 593 | Paragraph.Number > 774)
# Find sections with less than 10 words, probably not real sections
section.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
group_by(section) %>%
count() %>%
filter(n < 10)
# Filter out sections with less than 10 words
baselineDoc_section <- filter(baselineDoc_section, !section %in% section.count$section)
word.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
count()
#### Build to find unbalanced sections ####
section.word.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
count(section, sort = TRUE)
write.csv(section.word.count, file = "Models/section_word_count.csv", row.names = FALSE)
#### Find important bigrams from the baseline document ####
bigram.baseline <- baselineDoc_section %>%
select(section, Document.ID, Text) %>%
ungroup() %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2) %>%
separate(bigram,
c("word1", "word2"),
sep = " ") %>%
anti_join(stop_words,
by = c("word1" = "word")) %>%
anti_join(stop_words,
by = c("word2" = "word")) %>%
anti_join(cms_stop,
by = c("word1" = "word")) %>%
anti_join(cms_stop,
by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ")
bigram.section.count <- bigram.baseline %>%
count(section, word, sort = TRUE)
bigram.section.total <- bigram.section.count %>%
group_by(section) %>%
summarize(total = sum(n))
bigram.count <- bigram.baseline %>%
count(word,
sort = TRUE)
bigram <- left_join(bigram.section.count,
bigram.section.total)
bigram.tf_idf <- bigram %>%
bind_tf_idf(word,
section,
n) %>%
arrange(desc(tf_idf)) %>%
select(-n) %>%
inner_join(bigram.count, by = c("word" = "word"))
bigram.list <- union(select(filter(bigram.count, n > 10), word),
select(filter(bigram.tf_idf, n > 5 & tf_idf > .1), word)) %>%
distinct() %>%
arrange(word)
write.csv(bigram.list, file = "Data/bigram_list2.csv", row.names = FALSE)
#### Integrate Bigrams ####
bigram.index <- baselineDoc_section %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2,
collapse = FALSE) %>%
rowid_to_column("index") %>%
separate(bigram,
c("word1", "word2"),
sep = " ") %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2))
bigram.last.row <- bigram.index %>%
group_by(Paragraph.Number) %>%
filter(row_number() == n()) %>%
ungroup() %>%
mutate(word = word2,
index = index + 1) %>%
filter(!is.na(word)) %>%
select(-c(word1, word2))
bigram.index <- bigram.index %>%
union_all(bigram.last.row) %>%
unite(word,
word1,
word2,
sep = " ") %>%
arrange(Paragraph.Number,
index) %>%
rowid_to_column("index1") %>%
select(-index) %>%
inner_join(bigram.list,
by = c("word" = "word")) %>%
mutate(index2 = index1 + 1,
index = index1)
#%>%
#    select(Document.ID, bigram, index1, index2)
#write.csv(bigram.index, file = "bigram_index.csv", row.names = FALSE)
baseline.unigram <- baselineDoc_section %>%
unnest_tokens(word,
Text) %>%
rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index,
by = c("index" = "index1")) %>%
anti_join(bigram.index,
by = c("index" = "index2")) %>%
mutate(word = wordStem(word))
#write.csv(baseline.unigram, file = "unigram_index.csv", row.names = FALSE)
baseline <- baseline.unigram %>%
union_all(bigram.index) %>%
select(-c(index1, index2)) %>%
arrange(Paragraph.Number)
# Similarity to Baseline Document Threshold
baseSimThres = .95
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv",
stringsAsFactors = FALSE) # dataset of comments
bigram.list <- read.csv("Data/bigram_list2.csv",
stringsAsFactors = FALSE) # load list of relevant bigrams
commentsDf$Text <- gsub("[^A-z0-9 ]", " ", commentsDf$Text)
#### Remove Comment Attachment that are Uploads of the Baseline Document ####
baseSim <- read.csv("Data/Similarity_to_Baseline.csv")
simRemoves <- baseSim %>%
filter(distance > baseSimThres) %>%
select(doc2) %>%
rename(doc = doc2)
commentsDf <- commentsDf %>%
filter(!(Document.ID %in% simRemoves$doc))
#### Create Corpus Using Tidy ####
data(stop_words) # load common English stop words
cms_stop <- read_csv("Data/cms_stop_words.csv") # load CMS-specific stop words
## Unnest tokens, integrate bigrams, and remove stop words ##
# Unnnest bigram tokens
bigram.index <- commentsDf %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2) %>% # unnest bigram tokens
rowid_to_column("index") %>% # add index column
separate(bigram,
c("word1", "word2"),
sep = " ") %>% # separate bigram into two words for stemming
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) # stem separated bigrams
# Fix bigram unnesting so it matches word unnesting. Bigram unnesting will not unnest the final word of a cell into the first word of the next cell or by itself so the indexing between bigrams and words will not naturally match. So, we are unnesting the second word from the final bigram from each Document.ID-Page.Number combo to create a separate row with the final word by itself
bigram.last.row <- bigram.index %>%
group_by(Document.ID,
Page.Number) %>% # group by Document.ID and Page.Number
filter(row_number() == n()) %>% # filters for the last row of each group
ungroup() %>%
mutate(word = word2, # transform the bigram into the final word
index = index + 1) %>% # increment the index so it arrange at the end of the Document.ID-Page.Number group when reintegrated
filter(!is.na(word)) %>% # remove NA that show up when unnesting text with less than two words
select(-c(word1, word2)) #
bigram.index <- bigram.index %>%
unite(word,
word1,
word2,
sep = " ") %>% # recombine stemmed bigrams
union_all(bigram.last.row) %>% # combine the original unnested bigram and the single word fix
arrange(Document.ID,
Page.Number,
index) %>% # re
rowid_to_column("index1") %>%
select(-index) %>%
inner_join(bigram.list,
by = c("word" = "word")) %>%
mutate(index2 = index1 + 1,
index = index1)
comment.words <- commentsDf %>%
unnest_tokens(word,
Text) %>%
rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index,
by = c("index" = "index1")) %>%
anti_join(bigram.index,
by = c("index" = "index2")) %>%
mutate(word = wordStem(word))
comments <- comment.words %>%
union_all(bigram.index) %>%
select(-c(index1, index2))
combined <- baseline %>%
ungroup() %>%
select(Document.ID = section, word) %>%
union_all(select(comments, Document.ID, word))
combined.count <- combined %>%
count(Document.ID,
word,
sort = TRUE)
combined.total <- combined.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
combined.tf <- left_join(combined.count,
combined.total)
combined.tf_idf <- combined.tf %>%
bind_tf_idf(word,
Document.ID,
n)
combined.dtm <- cast_dtm(combined.tf_idf,
Document.ID,
word,
n)
