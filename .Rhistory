spread(document, score) # Transform tidy dataframe into wide format
# Write out results
write.csv(words_scoring.matrix, file = "Data/results.csv", row.names = FALSE)
# Create histograms of scores for each section to look at distribution
for (i in 1:length(unique(words_scoring.norm$document))) {
doc <- unique(words_scoring.norm$document)[i]
subset <- words_scoring.norm %>%
filter(document == doc) %>%
filter(score > .01)
png(paste0("score_hists/", doc,".png"), width=1280,height=800)
hist(subset$score, main = doc)
dev.off()
}
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
words_dtm_tf_idf <- cast_dtm(comment.tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
topic_map <- read.csv(file = "Models/topic_map.csv")
model.doc.term <- read.csv(file="Models/modelingTopicsTermsBeta.csv")
model.doc.topic <- read.csv(file="Models/modelingDocsTopicsGamma.csv")
# Run model against comments
words_dtm.topics <- posterior(base_lda, words_dtm)
# Transform results into tidy dataframe and map scores to document sections
words_scoring <- as.data.frame(cbind(Document.ID = rownames(words_dtm.topics$topics),
words_dtm.topics$topics)) %>%
gather(key = "Topic", value = "Score", 2:91) %>%
mutate(Topic = as.numeric(Topic)) %>%
right_join(model.doc.topic, by = c("Topic" = "topic")) %>%
mutate(final_score = as.numeric(Score) * gamma) %>%
group_by(Document.ID, document) %>%
summarise(sum = sum(final_score)/sum(gamma)) %>%
arrange(-sum) %>%
ungroup()
# Calculate min and max score of each section to normalize scores
minmax.base <- words_scoring %>%
group_by(document) %>%
summarise(min = min(sum, na.rm = TRUE),
max = max(sum, na.rm = TRUE))
# Mix-Max normalize scores
words_scoring.norm <- words_scoring %>%
inner_join(minmax.base, by = c("document" = "document")) %>%
mutate(score = (sum-min)/(max-min))
# Transform dataframe for final use
words_scoring.matrix <- words_scoring.norm %>%
mutate(score = if_else(score < .001, 0, score)) %>% # Scores below .1% are changed to zero to remove scientific notation
select(-c(sum, min, max)) %>% # Remove unneeded scoring components
spread(document, score) # Transform tidy dataframe into wide format
# Write out results
write.csv(words_scoring.matrix, file = "Data/results.csv", row.names = FALSE)
# Create histograms of scores for each section to look at distribution
for (i in 1:length(unique(words_scoring.norm$document))) {
doc <- unique(words_scoring.norm$document)[i]
subset <- words_scoring.norm %>%
filter(document == doc) %>%
filter(score > .01)
png(paste0("score_hists/", doc,".png"), width=1280,height=800)
hist(subset$score, main = doc)
dev.off()
}
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) # add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") # local location of github repo
source("helper_functions.R")
# Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(topicmodels)
library(quanteda)
library(visNetwork)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
bigram.list <- read.csv("Data/bigram_list2.csv", stringsAsFactors = FALSE)
#### Create Corpus Using Tidy ####
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#Unnest tokens and remove stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
#### Create TF-IDF ####
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
baseline <- read_csv("baseline.csv")
View(baseline)
View(commentsDf)
baselineDoc <- read_excel("Data/AN_Part2 - V4.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
View(baselineDoc)
#Unnest tokens and remove stop words
docs <- commentsDf %>%
select(Document.ID, Text) %>%
union_all(select(baselineDoc,
Document.ID,
Text))
comment.words <- docs %>%
unnest_tokens(word, Text) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
baselineName <- unique(baselineDoc$Document.ID)
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
words_dfm <- cast_dfm(comment.count,
document = Document.ID,
term = word,
value = n)
words_dfm_tf_idf <- cast_dfm(comment.tf_idf,
document = Document.ID,
term = word,
value = tf_idf)
words_dtm <- cast_dtm(comment.tf_idf,
Document.ID,
word,
n)
# Calculate similarity matrix and tidy it up
similarity <- textstat_simil(words_dfm,
margin = "documents",
method = "cosine") %>%
as.dist() %>%
tidy() %>%
arrange(desc(distance))
names(similarity) <- c("doc1", "doc2", "distance")
head(similarity)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc1 == baselineName)
head(baseSim)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName)
head(baseSim)
hist(baseSim$distance)
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName) %>%
arrange(-distance)
head(baseSim)
similarity <- similarity %>%
filter(!(doc1 == baselineName | doc2 == baselineName))
?rename
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc1 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc2 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim <- union_all(baseSim1,
baseSim2)
View(baseSim)
unique(baseSim$doc1)
baseSim <- baseSim1 %>%
union_all(baseSim2) %>%
arrange(-distance)
View(baseSim)
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) # add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") # local location of github repo
source("helper_functions.R")
# Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(topicmodels)
library(quanteda)
library(visNetwork)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
bigram.list <- read.csv("Data/bigram_list2.csv", stringsAsFactors = FALSE)
baselineDoc <- read_excel("Data/AN_Part2 - V4.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
#### Create Corpus Using Tidy ####
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#Unnest tokens and remove stop words
docs <- commentsDf %>%
select(Document.ID, Text) %>%
union_all(select(baselineDoc,
Document.ID,
Text))
baselineName <- unique(baselineDoc$Document.ID)
comment.words <- docs %>%
unnest_tokens(word, Text) %>%
#    filter(!(str_detect(word, regex("^http"))),
#           !(str_detect(word, regex("^www")))) %>%
#    anti_join(stop_words) %>%
#    anti_join(cms_stop) %>%
#    mutate(word = wordStem(word))
#### Create TF-IDF ####
comment.count <- comment.words %>%
count(Document.ID,
word,
sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count,
comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word,
Document.ID,
n)
#### Convert to DFM and Calculate Similarity Matrices ####
words_dfm <- cast_dfm(comment.count,
document = Document.ID,
term = word,
value = n)
words_dfm_tf_idf <- cast_dfm(comment.tf_idf,
document = Document.ID,
term = word,
value = tf_idf)
words_dtm <- cast_dtm(comment.tf_idf,
Document.ID,
word,
n)
# Calculate similarity matrix and tidy it up
similarity <- textstat_simil(words_dfm,
margin = "documents",
method = "cosine") %>%
as.dist() %>%
tidy() %>%
arrange(desc(distance))
names(similarity) <- c("doc1", "doc2", "distance")
head(similarity)
#### Separate out baseline similarities ####
baseSim <- similarity %>%
filter(doc1 == baselineName | doc2 == baselineName) %>%
arrange(-distance)
baseSim1 <- filter(baseSim, doc1 == baselineName)
baseSim2 <- filter(baseSim, doc2 == baselineName) %>%
rename(doc1 = doc2,
doc2 = doc1)
baseSim <- baseSim1 %>%
union_all(baseSim2) %>%
arrange(-distance)
unique(baseSim$doc1)
similarity <- similarity %>%
filter(!(doc1 == baselineName | doc2 == baselineName))
View(baseSim)
write.csv(baseSim,
file = "Data/Similarity_to_Baseline.csv",
row.names = FALSE)
# Similarity to Baseline Document Threshold
baseSimThres = .95
simRemoves <- baseSim %>%
filter(distance > baseSimThres) %>%
select(doc2) %>%
rename(doc2 = doc)
simRemoves <- baseSim %>%
filter(distance > baseSimThres) %>%
select(doc2) %>%
rename(doc = doc2)
View(simRemoves)
filter(!(Document.ID %in% simRemoves$doc)
commentsDf <- commentsDf %>%
commentsDf <- commentsDf %>%
)
commentsDf <- commentsDf %>%
filter(!(Document.ID %in% simRemoves$doc))
View(commentsDf)
# Load comment data
commentsDf <- read.csv("Data/commentsDf.csv",
stringsAsFactors = FALSE) # dataset of comments
test <- commentsDf %>%
filter((Document.ID %in% simRemoves$doc))
commentsDf <- commentsDf %>%
filter(!(Document.ID %in% simRemoves$doc))
#### Investigate similar comments ####
hist(filter(similarity, distance > .85)$distance)
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
baseStart = 66
# Load libraries
library(tidyverse)
library(readxl)
library(tidytext)
library(tm)
library(SnowballC)
library(topicmodels)
library(quanteda)
# Load baseline document for building model
baselineDoc <- read_excel("Data/AN_Part2 - V4.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
# Load stop words
data(stop_words)
# Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
#baselineDoc$Text <- gsub("[^A-z0-9 ]", " ", baselineDoc$Text)
# Add section column and removes lines paragraphs that are mostly numbers, which are generally parts of tables
baselineDoc_section <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, ifelse(str_detect(Text, regex("^attachment [A-Z]", ignore_case = TRUE)), Text, NA))) %>%
mutate(Text = str_replace_all(Text, "-", " ")) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
filter(str_detect(section, regex("^section [A-Z]", ignore_case = TRUE)))
# Remove table of contents for call letter
baselineDoc_section <- baselineDoc_section %>%
filter(Paragraph.Number < 514 | Paragraph.Number > 695)
# Find sections with less than 10 words, probably not real sections
section.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
group_by(section) %>%
count() %>%
filter(n < 10)
# Filter out sections with less than 10 words
baselineDoc_section <- filter(baselineDoc_section, !section %in% section.count$section)
word.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
count()
#### Build to find unbalanced sections ####
section.word.count <- baselineDoc_section %>%
unnest_tokens(word, Text) %>%
count(section, sort = TRUE)
write.csv(section.word.count, file = "Models/section_word_count.csv", row.names = FALSE)
#### Find important bigrams from the baseline document ####
bigram.baseline <- baselineDoc_section %>%
select(section, Document.ID, Text) %>%
ungroup() %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2) %>%
separate(bigram,
c("word1", "word2"),
sep = " ") %>%
anti_join(stop_words,
by = c("word1" = "word")) %>%
anti_join(stop_words,
by = c("word2" = "word")) %>%
anti_join(cms_stop,
by = c("word1" = "word")) %>%
anti_join(cms_stop,
by = c("word2" = "word")) %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2)) %>%
unite(word, word1, word2, sep = " ")
bigram.section.count <- bigram.baseline %>%
count(section, word, sort = TRUE)
bigram.section.total <- bigram.section.count %>%
group_by(section) %>%
summarize(total = sum(n))
bigram.count <- bigram.baseline %>%
count(word,
sort = TRUE)
bigram <- left_join(bigram.section.count,
bigram.section.total)
bigram.tf_idf <- bigram %>%
bind_tf_idf(word,
section,
n) %>%
arrange(desc(tf_idf))
bigram.list <- union(select(filter(bigram.count, n > 10), word),
select(filter(bigram.tf_idf, n > 5 & tf_idf > .1), word)) %>%
distinct() %>%
arrange(word)
write.csv(bigram.list, file = "Data/bigram_list2.csv", row.names = FALSE)
#### Integrate Bigrams ####
bigram.index <- baselineDoc_section %>%
unnest_tokens(bigram,
Text,
token = "ngrams",
n = 2,
collapse = FALSE) %>%
rowid_to_column("index") %>%
separate(bigram,
c("word1", "word2"),
sep = " ") %>%
mutate(word1 = wordStem(word1),
word2 = wordStem(word2))
bigram.last.row <- bigram.index %>%
group_by(Paragraph.Number) %>%
filter(row_number() == n()) %>%
ungroup() %>%
mutate(word = word2,
index = index + 1) %>%
filter(!is.na(word)) %>%
select(-c(word1, word2))
bigram.index <- bigram.index %>%
union_all(bigram.last.row) %>%
unite(word,
word1,
word2,
sep = " ") %>%
arrange(Paragraph.Number,
index) %>%
rowid_to_column("index1") %>%
select(-index) %>%
inner_join(bigram.list,
by = c("word" = "word")) %>%
mutate(index2 = index1 + 1,
index = index1)
#%>%
#    select(Document.ID, bigram, index1, index2)
#write.csv(bigram.index, file = "bigram_index.csv", row.names = FALSE)
baseline.unigram <- baselineDoc_section %>%
unnest_tokens(word,
Text) %>%
rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index,
by = c("index" = "index1")) %>%
anti_join(bigram.index,
by = c("index" = "index2")) %>%
mutate(word = wordStem(word))
#write.csv(baseline.unigram, file = "unigram_index.csv", row.names = FALSE)
baseline <- baseline.unigram %>%
union_all(bigram.index) %>%
select(-c(index1, index2)) %>%
arrange(Paragraph.Number)
write.csv(baseline, file = "baseline.csv", row.names = FALSE)
baseline.count <- baseline %>%
count(section,
word,
sort = TRUE)
baseline.total <- baseline.count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(baseline.count,
baseline.total)
baseline.tf_idf <- base %>%
bind_tf_idf(word,
section,
n)
baseline.dtm <- cast_dtm(baseline.tf_idf,
section,
word,
n)
#### Train topic model ####
base_lda <- LDA(baseline.dtm,
k = 79,
control = list(seed = 1234))
save(base_lda, file = "Models/lda_test.rda")
model.topic.term <- tidy(base_lda, matrix = "beta")
model.doc.topic <- tidy(base_lda, matrix = "gamma")
model.doc.term <- model.topic.term %>%
inner_join(model.doc.topic,
by = c("topic" = "topic")) %>%
mutate(score = beta * gamma) %>%
select(document, term, score, gamma) %>%
group_by(document,
term) %>%
summarise(score = sum(score)/sum(gamma)) %>%
arrange(desc(score))
View(model.doc.term)
View(model.topic.term)
test <- model.topic.term %>%
filter(topic == 1)
sum(test$beta)
test <- model.topic.term %>%
filter(topic == 50)
sum(test$beta)
