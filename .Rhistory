.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
#Load libraries
library(tidyverse)
library(readxl)
library(tidytext)
library(tm)
library(SnowballC)
baselineDoc <- read_excel("Data/AN_Part2.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
data(stop_words)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number > 65) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
View(testing)
#Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(hunspell)
#Load comment data
commentsDf <- read.csv("Data/commentsDf.csv",
stringsAsFactors = FALSE)
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- data.frame(word = c("CMS","Medicare","MA", "plan", "care", "beneficiaries", "Advantage", "proposed", "rule", "health", "plans", "MD", "Baltimore", "Seema", "Verma", "Washington", "DC", "boulevardbaltimore"), stringsAsFactors = FALSE)
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text, to_lower = FALSE) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www"))))
spell.find <- hunspell_find(comment.words$word)
spell.count <- as.data.frame(matrix(unlist(spell.find)),
byrow = TRUE,
stringsAsFactors = FALSE)
spell.count.unique <- as.data.frame(matrix(unique(unlist(spell.find)),
byrow = TRUE),
stringsAsFactors = FALSE)
spell.suggest <- hunspell_suggest(spell.count.unique$V1)
#Function to find the most amount of elements in a list
elementMax <- function(list){
maxEl = 0
for (i in 1:length(list)){
if (length(list[[i]]) > maxEl) {
maxEl <- length(list[[i]])
}
}
return(maxEl)
}
#Create blank data.frame to input spelling suggestions into
spell.suggest.df <- data.frame(word = matrix(unlist(spell.count.unique), byrow = TRUE),
matrix(nrow = length(spell.suggest),
ncol = elementMax(spell.suggest)))
#Put spelling suggestions into a data frame
for (i in 1:14652){
if (length(spell.suggest[[i]]) != 0){
for (j in 1:length(spell.suggest[[i]])){
spell.suggest.df[i, j+1] = spell.suggest[[i]][j]
}
}
}
#Create blank data.frame to input spelling find results into
spell.find.df <- data.frame(Document.ID = comment.words$Document.ID,
word = comment.words$word,
misspell = matrix(nrow = dim(comment.words)[1], ncol = 1))
for (i in 1:length(spell.find)){
if (length(spell.find[[i]]) != 0){
spell.find.df[i, 3] = spell.find[[i]][1]
}
}
spell.count.overall <- spell.find.df %>%
group_by(Document.ID) %>%
count()
spell.count.find <- spell.find.df %>%
filter(!is.na(misspell)) %>%
group_by(Document.ID) %>%
count()
spell.count.overall <- spell.count.overall %>%
left_join(spell.count.find, by = c("Document.ID" = "Document.ID"))
spell.count.overall$n.y <- spell.count.overall$n.y %>%
replace_na(0)
spell.count.overall <- spell.count.overall %>%
mutate(percent.misspelled = n.y/n.x) %>%
arrange(desc(percent.misspelled))
write.csv(spell.count.overall,
file = "Data/spellCheckMetric.csv",
row.names = FALSE)
spell.count.overall %>%
ggplot(aes(x = percent.misspelled)) +
geom_density()
spell.count.overall %>%
filter(n.x < 2000) %>%
ggplot(aes(x = n.x)) +
geom_density()
spell.count.overall %>%
ggplot(aes(x = percent.misspelled)) +
geom_density()
spell.count.overall %>%
filter(n.x < 2000) %>%
ggplot(aes(x = n.x)) +
geom_density()
spell.count.overall %>%
filter(n.x < 100) %>%
ggplot(aes(x = n.x)) +
geom_density()
View(spell.count.overall)
spell.count.overall %>%
filter(n.x < 100 & n.x > 6) %>%
ggplot(aes(x = n.x)) +
geom_density()
spell.count.hist <- spell.count.overall %>%
group_by(n) %>%
count() %>%
ungroup()
View(spell.count.overall)
spell.count.hist <- spell.count.overall %>%
group_by(n.x) %>%
count() %>%
ungroup()
View(spell.count.hist)
View(spell.count.overall)
#### Check for word cutoff ####
count.check <- commentsDf %>%
inner_join(spell.count.overall) %>%
arrange(n.x)
View(count.check)
#### Check for word cutoff ####
count.check <- commentsDf %>%
inner_join(spell.count.overall) %>%
arrange(n.x) %>%
select(Document.ID, Text, n.x, n.y, percent.misspelled)
View(count.check)
write.csv(count.check, file = "Data/countCheck.csv", row.names = FALSE)
testComment <- commentsDf %>%
filter(!(word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")))
testComment <- commentsDf %>%
filter((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")))
View(testComment)
View(testComment)
write.csv(testComment, file = "Data/testAttacted.csv")
testComment <- commentsDf %>%
filter((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached") & wordcount(Text) > 150))
wordcount <- function(str) {
sapply(gregexpr("\\b\\W+\\b", str, perl=TRUE), function(x) sum(x>0) ) + 1
}
testComment <- commentsDf %>%
filter((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached") & wordcount(Text) > 150))
View(testComment)
testComment <- commentsDf %>%
filter(!(word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached") & wordcount(Text) > 150))
testComment <- commentsDf %>%
filter(!((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")) & wordcount(Text) > 150))
testComment <- commentsDf %>%
filter(!((word(Document.ID, -1, sep = "-") == "0" & str_detect(tolower(Text), "attached")) & wordcount(Text) < 150))
View(commentsDf)
