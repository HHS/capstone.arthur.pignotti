pageUrl = paste("https://api.data.gov:443/regulations/v3/documents.json?api_key=", api_key, "&rpp=100&dct=PS&encoded=1&dktid=", dktid, "&po=", (i-1)*100, sep ="")
dataPull <- fromJSON(pageUrl)
if (i==1){
comments <- data.frame(dataPull$documents)
} else {
tmp <- data.frame(dataPull$documents)
comments <- rbind(comments, tmp)
}
}
dfCorpus = Corpus(VectorSource(comments$commentText))
#libraries
library(tm)
mycorpus = Corpus(VectorSource(comments$commentText))
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
View(comments)
View(mycorpus)
mycorpus[["4"]][["content"]]
View(comments)
attachList <- comments[comments$attachmentCount>0]
attachList <- subset(comments, attachmentCount > 0)
View(attachList)
?download.file
testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
library(RCurl)
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
testUrl
library(httr)
test <- HEAD(testUrl)
View(test)
test$content
test$all_headers
test$headers$`content-disposition`
test$headers$content-disposition
tmp <- test$headers$`content-disposition`
file = sub(".=\", """, tmp)
?sub
file = sub(".=\", tmp)
download.file(attachUrl, file, mode="wb")
}
}
library(httr)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
myfiles <- list.files(path = comLoc, pattern = "pdf",  full.names = TRUE) #get list of PDFs
comReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Nonrulemaking-Public Submission")
attReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Attachments")
#Placeholder for loading html files
mycorpus <- Corpus(DirSource(comLoc, pattern = "html"))
myhtmlfiles <- list.files(path = comLoc, pattern = "html",  full.names = TRUE) #get list of PDFs
mycorpus = Corpus(VectorSource(comments$commentText))
mycorpus <- tm_map(mycorpus, stripWhitespace)
mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus <- tm_map(mycorpus, removeWords, stopwords("english"))
#html test
webpage <- read_html(myhtmlfiles[1])
test <- html_table(html_nodes(webpage, "table"))
file = sub(".=\", tmp)
download.file(attachUrl, file, mode="wb")
}
}
library(httr)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
myfiles <- list.files(path = comLoc, pattern = "pdf",  full.names = TRUE) #get list of PDFs
comReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Nonrulemaking-Public Submission")
attReport <- read_excel("CMS-2017-0163/report.xlsx", sheet = "Attachments")
#Placeholder for loading html files
mycorpus <- Corpus(DirSource(comLoc, pattern = "html"))
file = substr(tmp,23,length(tmp))
file = substr(tmp,23)
length(tmp)
file = substr(tmp,23,nchar(tmp))
file
file = substr(tmp,23,nchar(tmp)-1)
file
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", document, sep ="")
testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(testUrl)
file <- substr(test$headers$`content-disposition`, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", document, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/", file, sep = ""), mode="wb")
}
}
attachList <- subset(comments, attachmentCount > 0)
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], " - " , file, sep = ""), mode="wb")
}
}
sum(attachList$attachmentCount)
nrow(attachList)
cnt = 0
?data.frame
?subset
?subset
?data.frame
?colnames
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText), row.names=c("doc_id","text",colnames(subset(comments, select = -c(documentId, commentText)))))
)
View(commentsDf)
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText))))
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""),comments$commentText,subset(comments, select = -c(documentId, commentText)))
View(commentsDf)
colnames(commentsDf[,1:2]) <- c("doc_id", "text")
View(commentsDf)
colnames(commentsDf[,1]) <- c("doc_id")
colnames(commentsDf[,1]) <- "doc_id"
colnames(commentsDf)[1] <- "doc_id"
View(commentsDf)
colnames(commentsDf)[1:2] <- c("doc_id","text")
mycorpus = Corpus(DataframeSource(commentsDf))
View(mycorpus)
########################
# Load FDMS Reports    #
########################
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
library(janeaustenr)
install.packages("janeaustenr", lib="C:/R/Packages")
library(janeaustenr)
library(dplyr)
library(stringr)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
View(original_books)
head(original_books)
library(tidytext)
install.packages("tidytext", lib="C:/R/Packages")
library(tidytext)
tidy_books <- original_books %>%
unnest_tokens(word, text)
tidy_books
View(tidy_books)
?unnest_tokens
austen_books()
original_books <- austen_books()
View(original_books)
test <- commentsDf %>%
unnest_tokens(word, text)
original_books <- austen_books() %>%
group_by(book) %>%
mutate(linenumber = row_number(),
chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup()
View(commentsDf)
test <- commentsDf %>%
group_by(doc_id) %>%
ungroup()
View(test)
test <- commentsDf %>%
group_by(doc_id) %>%
mutate(linenumber = row_number()) %>%
ungroup()
test1 <- test %>%
unnest_tokens(word, text)
str(test)
str(original_books)
str(comments)
commentsDf <- data.frame(paste(comments$documentId,"-0",sep=""), comments$commentText, stringsAsFactors = FALSE)
colnames(commentsDf) <- c("doc_id","text")
mycorpus = Corpus(DataframeSource(commentsDf))
test1 <- test %>%
unnest_tokens(word, text)
test <- commentsDf %>%
group_by(doc_id) %>%
mutate(linenumber = row_number()) %>%
ungroup()
test1 <- test %>%
unnest_tokens(word, text)
View(test1)
data(stop_words)
test1 <- test1 %>%
anti_join(stop_words)
test1 %>%
count(word, sort = TRUE)
tidy_books %>%
count(word, sort = TRUE)
library(dplyr)
tidy_books %>%
count(word, sort = TRUE)
tidy_books %>%
dplyr::count(word, sort = TRUE)
library(ggplot2)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
test1 %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
tidy_books <- tidy_books %>%
anti_join(stop_words)
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(gutenbergr)
install.packages("gutenbergr", lib="C:/R/Packages")
library(gutenbergr)
gutenberg_metadata
tmp <- gutenberg_metadata
View(tmp)
dumas <- subset(tmp, author="Dumas, Alexandre")
dumas <- subset(tmp, author=="Dumas, Alexandre")
View(dumas)
?subset
dumas <- subset(tmp, author=="Dumas, Alexandre", language=="en")
dumas <- subset(tmp, author=="Dumas, Alexandre" & language=="en")
tidy_books %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
test1 %>%
count(word, sort = TRUE) %>%
filter(n > 600) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
myfiles <- list.files(path = "files/", pattern = "pdf",  full.names = TRUE) #get list of PDFs
attachList <- subset(comments, attachmentCount > 0)
cnt = 0
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], "-", doc, " - " , file, sep = ""), mode="wb")
cnt=cnt+1
}
}
View(attachList)
?write.csv
write.csv(attachList, file="test.csv")
nrow(attachList)
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=CMS-2017-0163-1203&disposition=attachment&attachmentNumber=1, sep ="")
)
attachUrl = "https://www.regulations.gov/contentStreamer?documentId=CMS-2017-0163-1203&disposition=attachment&attachmentNumber=1"
test <- HEAD(attachUrl)
View(test)
test <- HEAD(attachUrl)
View(test)
cnt = 0
?data.frame
errflag = 1
for (comment in 1:nrow(attachList)){
for (doc in 1:attachList[comment, "attachmentCount"]){
attachUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[comment, "documentId"], "&disposition=attachment&attachmentNumber=", doc, sep ="")
#testUrl = paste("https://www.regulations.gov/contentStreamer?documentId=", attachList[1, "documentId"], "&disposition=attachment&attachmentNumber=", 1, sep ="")
#test <- getURI(testUrl,header=TRUE,verbose=TRUE)
test <- HEAD(attachUrl)
if (test$status_code==200){
tmp <- test$headers$`content-disposition`
file <- substr(tmp, 23, nchar(tmp)-1)
download.file(attachUrl, paste("files/",  attachList[comment, "documentId"], "-", doc, " - " , file, sep = ""), mode="wb")
} else if (errflag==1){
errorLog = data.frame(docId = attachList[comment, "documentId"], error = test$status_code)
errflag = errflag+1
} else {
errorLog[errflag, 1] = attachList[comment, "documentId"]
errorLog[errflag, 2] = error = test$status_code
errflag = errflag+1
}
}
}
View(errorLog)
get_sentiments("afinn")
nrc_joy <- get_sentiments("nrc") %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
test_sentiment <- test1 %>%
inner_join(get_sentiments("bing")) %>%
count(sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
ggplot(jane_austen_sentiment, aes(index, sentiment)) +
geom_col(show.legend = FALSE) +
library(gutenbergr)
ggplot(jane_austen_sentiment, aes(index, sentiment)) +
geom_col(show.legend = FALSE)
bing_word_counts <- test1 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts %>%
group_by(sentiment) %>%
top_n(10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(y = "Contribution to sentiment",
x = NULL) +
coord_flip()
library(rvest)
.libPaths( c("C:/R/Packages", .libPaths()) )
.libPaths( c("C:/R/Packages", .libPaths()) )
library(rvest)
url <- 'https://www.uhc.com/'
webpage <- read_html(url)
View(webpage)
test <- webpage$doc
webpage$doc
test <- webpage$node
test <- webpage$node[[1]]
vignette("selectorgadget")
webpage %>%
html_node("title")
webpage %>%
html_node("title") %>%
html_text() %>%
title
webpage %>%
html_node("title") %>%
html_text() %>%
htitle
webpage %>%
html_node("title") %>%
html_text()
htitle <- webpage %>%
html_node("title") %>%
html_text()
siteDf <- subset(comReport$Site_Key, !(Site_Key %in% c(NA,"aol.com","google.com"))
)
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
library(readxl)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
siteDf <- subset(comReport$Site_Key, !(Site_Key %in% c(NA,"aol.com","google.com")))
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
siteDf <- subset(comReport$Site_Key, !(Site_Key %in% c(NA,"aol.com","google.com")))
siteDf <- subset(comReport, !(Site_Key %in% c(NA,"aol.com","google.com")))$Site_key
View(comReport)
siteDf <- subset(comReport, !(Site_Key %in% c(NA,"aol.com","google.com")))
View(siteDf)
siteDf <- subset(comReport, !(Site_Key %in% c(NA,"aol.com","google.com")))["Site_Key"]
View(siteDf)
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
'comcast.com',
'cox.net',
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(Site_Key %in% pubSites))["Site_Key"]
View(siteDf)
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))["Site_Key"]
View(siteDf)
test <- unique(siteDf$Site_Key)
test{1}
test[1]
url <- 'uhc.com/'
webpage <- read_html(url)
url <- 'www.uhc.com/'
webpage <- read_html(url)
url <- 'http://www.uhc.com/'
webpage <- read_html(url)
siteDF <- siteDf[!duplicated(siteDF$Site_Key)]
siteDF <- siteDf[!duplicated(siteDf$Site_Key)]
siteDf <- distinct(siteDf)
########################
# Load FDMS Reports    #
########################
library(tidyverse)
siteDf <- distinct(siteDf)
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))["Site_Key"]
siteDf <- distinct(siteDf)
for (i in 11:20){
url <- paste('http://www.', siteDf[i, 'SiteKey'], sep='')
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()
}
for (i in 11:20){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()
}
