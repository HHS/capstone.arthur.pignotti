install.packages("caret")
install.packages("sqldf")
install.packages("devtools")
install.packages("jsonlite")
install.packages("pdftools")
library(pdftools)
library("pdftools", lib.loc="C:/R/Packages")
library("ggplot2", lib.loc="C:/R/Packages")
library("codetools", lib.loc="C:/Program Files/R/R-3.3.3/library")
library("colorspace", lib.loc="C:/R/Packages")
library("crayon", lib.loc="C:/R/Packages")
.libPaths()
install.packages("ggplot2")
library(ggplot2)
install.packages("Rcpp")
install.packages("Rcpp")
library(ggplot2)
library("pdftools", lib.loc="C:/R/Packages")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
nvsr65_05 <- pdf_text("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf")
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[ [  1 ] ], "\n")[ [ 1 ] ])
head(strsplit(nvsr65_05[[1]], "\n")[[1]])
install.packages("tesseract")
library(tesseract)
eng <- tesseract("eng")
text <- tesseract::ocr("http://jeroen.github.io/images/testocr.png", engine = eng)
text <- tesseract::ocr("http://www.cdc.gov/nchs/data/nvsr/nvsr65/nvsr65_05.pdf", engine = eng)
text <- tesseract::ocr("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
results <- tesseract::ocr_data("C:/Data/Comments/CMS-2017-0163/Attachments/CMS-2017-0163-DRAFT-0411-A1.pdf", engine = eng)
print(text, n = 20)
print(results, n = 20)
View(results)
head(results.confidence)
summary(results$confidence)
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("shiny", lib="C:/R/Packages")
install.packages("yhat", lib="C:/R/Packages")
install.packages("stringr", lib="C:/R/Packages")
install.packages("forecast", lib="C:/R/Packages")
install.packages("randomForest", lib="C:/R/Packages")
install.packages("reshape2", lib="C:/R/Packages")
install.packages("lubridate", lib="C:/R/Packages")
install.packages("data.table", lib="C:/R/Packages")
install.packages("googleVis", lib="C:/R/Packages")
install.packages("caret", lib="C:/R/Packages")
install.packages("car", lib="C:/R/Packages")
install.packages("tm", lib="C:/R/Packages")
install.packages("twitteR", lib="C:/R/Packages")
install.packages("gbm", lib="C:/R/Packages")
install.packages("e1071", lib="C:/R/Packages")
install.packages("glmnet", lib="C:/R/Packages")
install.packages("tau", lib="C:/R/Packages")
install.packages("knitr", lib="C:/R/Packages")
install.packages("NLP", lib="C:/R/Packages")
.libPaths( c("C:/R/Packages", .libPaths()) )
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tidyverse)
########################
# Load FDMS Reports    #
########################
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS/files"
########################
# Load FDMS Reports    #
########################
library(readxl)
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
warnings()
unique(comReport)
count(comReport)
test <- unique(comReport)
View(test)
test <- count(comReport)
View(test)
test <- unique(comReport$Attachments)
comLoc <- "C:/Data/Comments/CMS-2017-0163/FDMS"
comReport <- read_excel(paste(comLoc, "/report.xlsx", sep=""), sheet = "Nonrulemaking-Public Submission")
View(comReport)
str(comReport)
test <- unique(comReport$`Organization Name`)
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
View(comReport)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
comReport$Site_Key <- substring(comReport$`Email Address`, regexpr("@", comReport$`Email Address`) + 1)
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))["Site_Key"]
siteDf <- distinct(siteDf)
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name"]
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
View(siteDf)
siteDf <- distinct(siteDf)
########################
# Load FDMS Reports    #
########################
library(tidyverse)
.libPaths( c("C:/R/Packages", .libPaths()) )
########################
# Load FDMS Reports    #
########################
library(tidyverse)
siteDf <- distinct(siteDf)
########################
# Web Scraping         #
########################
library(rvest)
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[1, 'Site_Key'], sep='')
webpage %>%
html_node("body") %>%
html_text()
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
########################
# Testing              #
########################
url <- paste('http://www.', siteDf[5, 'Site_Key'], sep='')
webpage <- read_html(url)
webpage %>%
html_node("body") %>%
html_text()
url
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
library(httr)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
########################
# Getting Info         #
########################
pubSites <- c(NA,
"aim.com",
"aol.com",
"google.com",
"gmail.com",
'comcast.com',
'cox.net',
"cox.com",
'hotmail.com',
'icloud.com',
'yahoo.com',
'mail.com',
'att.net',
'bellsouth.net',
'charter.net',
'comcast.net',
"msn.com",
'gmail.co',
'outlook.com',
'verizon.net',
'ymail.com',
'me.com',
'aeneas.net')
siteDf <- subset(comReport, !(tolower(Site_Key) %in% pubSites))[c("Site_Key","Organization Name")]
siteDf <- distinct(siteDf)
for (i in 1:nrow(siteDf)){
url <- paste('http://www.', siteDf[i, 'Site_Key'], sep='')
test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
test <- HEAD(url)
test <- http_error(url)
test <- url_success(url)
test <- !http_error(url)
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tid)
library(tidyverse)
tidyverse_update()
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("stm", lib="C:/R/Packages")
install.packages("quanteda", lib="C:/R/Packages")
#### Initial Setup ####
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
source("helper_functions.R")
#Load libraries
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
library(SnowballC)
library(tm)
library(topicmodels)
library(quanteda)
#Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
bigram.list <- read.csv("Data/bigram_list2.csv", stringsAsFactors = FALSE)
#### Create Corpus Using Tidy ####
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
commentsDf$Text <- gsub("[^A-z0-9 ]", "", commentsDf$Text)
#Unnest tokens and removd stop words
comment.bigrams <- commentsDf %>%
unnest_tokens(word, Text, token = "ngrams", n = 2) %>%
filter(word %in% bigram.list$word)
bigram.index <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
rowid_to_column("index1") %>%
inner_join(bigram.list, by = c("bigram" = "word")) %>%
mutate(index2 = index1 + 1) %>%
select(Document.ID, bigram, index1, index2)
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
rowid_to_column("index") %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www")))) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
anti_join(bigram.index, by = c("index" = "index1")) %>%
anti_join(bigram.index, by = c("index" = "index2")) %>%
mutate(word = wordStem(word)) %>%
select(-index)
comments <- union(comment.words, comment.bigrams)
#### TF-IDF Testing ####
comment.count <- comments %>%
mutate(Document.ID = paste0(Document.ID,"-",Page.Number)) %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
comment.total <- comment.count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
comment <- left_join(comment.count, comment.total)
comment.tf_idf <- comment %>%
bind_tf_idf(word, Document.ID, n)
comment.tf_idf %>%
filter(total > 8000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
words_dtm_tf_idf <- cast_dtm(comment.tf_idf, document = Document.ID, term = word, value = tf_idf)
words_dtm <- cast_dtm(comment.count, document = Document.ID, term = word, value = n)
#### Convert to DFM and Calculate Similarity Matrices ####
words_dfm <- cast_dfm(comment.count, document = Document.ID, term = word, value = n)
words_dfm_tf_idf <- cast_dfm(comment.tf_idf, document = Document.ID, term = word, value = tf_idf)
# Calculate similarity matrix and tidy it up
similarity <- textstat_simil(words_dfm,
margin = "documents",
method = "cosine") %>%
as.dist() %>%
tidy() %>%
arrange(desc(distance))
names(similarity) <- c("doc1", "doc2", "distance")
head(similarity)
library(igraph)
library(ggraph)
set.seed(2017)
bigram_graph <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 1000) %>%
graph_from_data_frame()
sim_graph <- similarity %>%
#    filter(distance > 1000) %>%
graph_from_data_frame()
sim_graph <- similarity %>%
filter(distance > .9) %>%
graph_from_data_frame()
a <- grid::arrow(type = "closed", length = unit(.1, "inches"))
ggraph(sim_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.01, 'inches')) +
geom_node_point(color = "lightblue", size = 3) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
bigram_graph <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
filter(n > 1000) %>%
graph_from_data_frame()
View(bigram_graph)
View(sim_graph)
a <- grid::arrow(type = "closed", length = unit(.01, "inches"))
ggraph(sim_graph, layout = "fr") +
geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
arrow = a, end_cap = circle(.01, 'inches')) +
geom_node_point(color = "lightblue", size = 1) +
geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
theme_void()
load("Models/lda_test.rda")
model.topic.term <- tidy(base_lda, matrix = "beta")
model.doc.topic <- tidy(base_lda, matrix = "gamma")
model.doc.term <- inner_join(model.topic.term, model.doc.topic, by = c("topic" = "topic")) %>%
mutate(score = beta * gamma) %>%
select(document, term, score) %>%
group_by(document, term) %>%
summarise(score = sum(score)) %>%
arrange(desc(score))
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta)) %>%
ungroup() %>%
group_by(document) %>%
top_n(20, final_beta) %>%
ungroup()
library(wordcloud)
subset <- model.topic.term %>%
filter(topic == 1)
wordcloud(subset$term, subset$beta)
subset <- model.topic.term %>%
filter(topic == 1 & beta > .01)
subset <- model.topic.term %>%
filter(topic == 1 & beta > .001)
wordcloud(subset$term, subset$beta)
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta)) %>%
ungroup() %>%
group_by(document) %>%
top_n(20, final_beta) %>%
ungroup()
View(doc.topic.terms)
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta)) %>%
ungroup() %>%
group_by(document) %>%
top_n(100, final_beta) %>%
ungroup()
subset <- model.topic.term %>%
filter(topic == unique(model.topic.term)[1])
wordcloud(subset$term, subset$final_beta)
View(subset)
subset <- model.topic.term %>%
filter(document == unique(model.topic.term)[1])
View(model.topic.term)
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta)) %>%
ungroup() %>%
group_by(document) %>%
top_n(100, final_beta) %>%
ungroup()
subset <- doc.topic.term %>%
filter(document == unique(model.topic.term)[1])
subset <- doc.topic.terms %>%
filter(document == unique(model.topic.term)[1])
subset <- doc.topic.terms %>%
filter(document == unique(model.topic.term)[2])
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[1])
wordcloud(subset$term, subset$final_beta)
?wordcloud
i=10
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=15
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=20
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta&gamma)) %>%
ungroup() %>%
group_by(document) %>%
top_n(100, final_beta) %>%
ungroup()
doc.topic.terms <- model.topic.term %>%
inner_join(model.doc.topic, by = c("topic" = "topic")) %>%
group_by(document, term) %>%
summarise(final_beta = sum(beta*gamma)) %>%
ungroup() %>%
group_by(document) %>%
top_n(100, final_beta) %>%
ungroup()
i=20
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=40
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=50
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=15
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=1
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=2
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=3
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
i=4
subset <- doc.topic.terms %>%
filter(document == unique(doc.topic.terms$document)[i])
print(unique(doc.topic.terms$document)[i])
wordcloud(subset$term, subset$final_beta)
doc <- unique(doc.topic.terms$document)[i]
for (i in 1:length(unique(doc.topic.terms$document))) {
doc <- unique(doc.topic.terms$document)[i]
subset <- doc.topic.terms %>%
filter(document == doc)
png(paste0("clouds/", doc,".png"), width=1280,height=800)
wordcloud(subset$term, subset$final_beta)
dev.off()
}
install.packages("RColorBrewer")
install.packages("RColorBrewer")
library(RColorBrewer)
pal2 <- brewer.pal(8,"Dark2")
for (i in 1:length(unique(doc.topic.terms$document))) {
doc <- unique(doc.topic.terms$document)[i]
subset <- doc.topic.terms %>%
filter(document == doc)
png(paste0("clouds/", doc,".png"), width=1280,height=800)
wordcloud(subset$term,
subset$final_beta,
scale=c(8,.3),
colors=pal2)
dev.off()
}
