test <- HEAD(url)
if (test$status_code==200){
webpage <- read_html(url)
siteDf[i, 'htitle'] <- webpage %>%
html_node("title") %>%
html_text()}
}
test <- HEAD(url)
test <- http_error(url)
test <- url_success(url)
test <- !http_error(url)
.libPaths( c("C:/R/Packages", .libPaths()) )
library(tid)
library(tidyverse)
tidyverse_update()
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
install.packages(c("dplyr", "purrr", "rlang", "tidyr"))
.libPaths( c("C:/R/Packages", .libPaths()) )
install.packages("stm", lib="C:/R/Packages")
install.packages("quanteda", lib="C:/R/Packages")
.libPaths( c("C:/R/Packages", .libPaths()) ) #add extra library location
setwd("C:/Users/P6BQ/Desktop/capstone.arthur.pignotti") #local location of github repo
library(dplyr)
library(stringr)
library(ggplot2)
library(tidytext)
library(tidyverse)
#Load comment data
commentsDf <- read.csv("Data/commentsDf.csv", stringsAsFactors = FALSE)
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- data.frame(word = c("cms","medicare","ma", "plan", "care", "beneficiaries", "advantage", "proposed", "rule", "health", "plans", "md", "baltimore", "seema", "verma", "washington", "dc", "boulevardbaltimore"), stringsAsFactors = FALSE)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
View(cms_stop)
#Unnest tokens and removd stop words
comment.words <- commentsDf %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
filter(!(str_detect(word, regex("^http"))),
!(str_detect(word, regex("^www"))))
#### TF-IDF Testing ####
word_count <- comment.words %>%
count(Document.ID, word, sort = TRUE) %>%
ungroup()
word_total <- word_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
words <- left_join(word_count, word_total)
words_tf_idf <- words %>%
bind_tf_idf(word, Document.ID, n)
words_tf_idf %>%
filter(total > 8000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 2, scales = "free") +
coord_flip()
words_tf_idf %>%
filter(total > 10000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 2, scales = "free") +
coord_flip()
words_tf_idf %>%
filter(total > 10000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
words_tf_idf %>%
filter(total > 8000) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 3, scales = "free") +
coord_flip()
bigram_count <- test %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ")
bigram_total <- bigram_count %>%
group_by(bigram) %>%
summarize(total = sum(n)) %>%
arrange(desc(total))
bigram_total <- bigram_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
bigrams <- left_join(bigram_count, bigram_total)
bigram_tf_idf <- bigrams %>%
bind_tf_idf(bigram, Document.ID, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
filter(total > 7000) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram))))%>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(bigram, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 2, scales = "free") +
coord_flip()
bigram_count <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(Document.ID, word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ")
bigram_total <- bigram_count %>%
group_by(bigram) %>%
summarize(total = sum(n)) %>%
arrange(desc(total))
bigram_total <- bigram_count %>%
group_by(Document.ID) %>%
summarize(total = sum(n))
bigrams <- left_join(bigram_count, bigram_total)
bigram_tf_idf <- bigrams %>%
bind_tf_idf(bigram, Document.ID, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
filter(total > 7000) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram))))%>%
group_by(Document.ID) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(bigram, tf_idf, fill = Document.ID)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~Document.ID, ncol = 2, scales = "free") +
coord_flip()
View(bigram_count)
View(bigram_total)
View(bigram_count)
bigram_phrase_count <- commentsDf %>%
unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
separate(bigram, c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
count(word1, word2, sort = TRUE) %>%
unite(bigram, word1, word2, sep = " ")
View(bigram_phrase_count)
View(bigram_tf_idf)
source("helper_functions.R")
library(SnowballC)
library(tm)
baselineDoc <- read_excel("Data/AN_Part2.xlsx", sheet = 1)
library(readxl)
baselineDoc <- read_excel("Data/AN_Part2.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
View(baselineDoc)
data(stop_words)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number > 65) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
View(testing)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
baseStart = 66
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
baselineDoc$Text <- gsub("^A-z0-9", "")
baselineDoc$Text <- gsub("^A-z0-9", "", baselineDoc$Text)
View(baselineDoc)
baselineDoc$Text <- gsub("[^A-z0-9]", "", baselineDoc$Text)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
View(testing)
baselineDoc <- read_excel("Data/AN_Part2.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
baselineDoc$Text <- gsub("[^A-z0-9 ]", "", baselineDoc$Text)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
fill(section)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
!filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    !filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
!filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE))))
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    !filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
#    !filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- testing %>%
!filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE)))
testing <- testing %>%
test = str_detect(Text, regex("^[A-z]", ignore_case = TRUE))
testing <- testing %>%
filter(str_detect(Text, regex("[A-z]", ignore_case = TRUE)))
View(testing)
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
#    !filter(str_detect(Text, regex("^[A-z]", ignore_case = TRUE))) %>%
fill(section)
testing <- testing %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5)
library(topicmodels)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
filter(Text != NA)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
filter(!is.na(Text))
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
filter(!is.na(section))
View(baseline)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
testing <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE))))
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section))
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
mutate(word = wordStem(word))
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
base_lda <- LDA(base_dtm, k = 2, control = list(seed = 1234))
View(base_lda)
str(base_lda)
View(base_dtm)
base_tf_idf %>%
filter(section %in% 9:12) %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
group_by(section) %>%
top_n(15) %>%
ungroup %>%
ggplot(aes(word, tf_idf, fill = section)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~section, ncol = 2, scales = "free") +
coord_flip()
View(base_tf_idf)
base_topics <- tidy(base_lda, matrix = "beta")
base_lda <- LDA(base_dtm, k = 10, control = list(seed = 1234))
base_topics <- tidy(base_lda, matrix = "beta")
model_top_terms <- base_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
model_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
base_lda <- LDA(base_dtm, k = 9, control = list(seed = 1234))
base_topics <- tidy(base_lda, matrix = "beta")
model_top_terms <- base_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
model_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-z0-9]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
base_lda <- LDA(base_dtm, k = 9, control = list(seed = 1234))
base_topics <- tidy(base_lda, matrix = "beta")
model_top_terms <- base_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
model_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
View(base_topics)
beta_spread <- base_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
View(beta_spread)
beta_spread <- base_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1)) %>%
order_by(abs(log_ratio))
beta_spread <- base_topics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1)) %>%
arrange(term, abs(log_ratio))
baselineDoc <- read_excel("Data/AN_Part2 - V2.xlsx", sheet = 1)
colnames(baselineDoc) <- make.names(colnames(baselineDoc))
View(baselineDoc)
#Load stop words
data(stop_words)
#Load CMS-specific stop words
cms_stop <- read_csv("Data/cms_stop_words.csv")
# Remove non-alpha-numeric characters
baselineDoc$Text <- gsub("[^A-z0-9 ]", "", baselineDoc$Text)
baseline <- baselineDoc %>%
group_by(Document.ID) %>%
filter(Text != "",
Paragraph.Number >= baseStart) %>%
mutate(section = ifelse(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)), Text, NA)) %>%
#    mutate(section = cumsum(str_detect(Text, regex("^section [A-Z]", ignore_case = TRUE)))) %>%
filter(str_count(Text, regex("[A-z]", ignore_case = TRUE))/str_count(Text, regex("[A-z0-9]", ignore_case = TRUE)) > .5) %>% # Removes lines that are 50% or more numbers
fill(section) %>%
filter(!is.na(section)) %>%
unnest_tokens(word, Text) %>%
anti_join(stop_words) %>%
anti_join(cms_stop) %>%
mutate(word = wordStem(word))
unique(baseline$section)
base_count <- baseline %>%
group_by(section) %>%
count(section, word, sort = TRUE) %>%
ungroup()
base_total <- base_count %>%
group_by(section) %>%
summarize(total = sum(n))
base <- left_join(base_count, base_total)
View(base)
base <- left_join(base_count, base_total) %>%
filter(total > 5)
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
count(unique(base$section))
unique(base$section)
dim(base$section)
?unique
count(unique(base$section))
test <- unique(base$section)
length(unique(base$section))
base_tf_idf <- base %>%
bind_tf_idf(word, section, n)
base_dtm <- cast_dtm(base_tf_idf, section, word, n)
base_lda <- LDA(base_dtm,
k = length(unique(base$section)),
control = list(seed = 1234))
base_topics <- tidy(base_lda, matrix = "beta")
View(base_topics)
train.topics <- topics(base_lda)
test.topics <- posterior(base_lda, base_dtm)
(test.topics <- apply(test.topics$topics, 1, which.max))
test.topics <- posterior(base_lda, base_dtm)
View(test.topics)
test.topics[["topics"]]
test <- tidy(test.topics)
test <- apply(test.topics$topics, 1, which.max)
